{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbHNmOvzjcwC"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTbCMkk8jcwK"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for data processing, ML, and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import kagglehub\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "import string\n",
        "import itertools\n",
        "\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d0FdWHolo_H"
      },
      "source": [
        "## Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVQ8Xif5lqil",
        "outputId": "daab0bd3-2a6e-409b-888d-a6528a5c62be"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR = \"./checkpoints\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", data_to_save=None):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -1}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "    previous_filename_json = f\"model_epoch_{epoch -1}_params_{hyperparameters}.json\"\n",
        "    previous_filepath_json = os.path.join(subfolder_path, previous_filename_json)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath) and os.path.exists(previous_filepath_json):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "    with open(filepath_json, 'w') as json_file:\n",
        "      json.dump(data_to_save, json_file, indent=4)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1, None  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca più alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Trova e carica il file JSON associato\n",
        "        json_filename = latest_file.replace('.pth', '.json')\n",
        "        json_filepath = os.path.join(subfolder_path, json_filename)\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(f\"JSON data loaded: {json_filepath}\")\n",
        "        else:\n",
        "            print(f\"No JSON file found for: {latest_file}\")\n",
        "\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1, None  # Le epoche iniziano da 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3i5KwH-pF1g"
      },
      "source": [
        "## Shakespeare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNrpQePKpJOS"
      },
      "outputs": [],
      "source": [
        "ALL_LETTERS = \"\\n !\\\"&'(),-.0123456789:;>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz}\"\n",
        "NUM_LETTERS = len(ALL_LETTERS)\n",
        "\n",
        "def letter_to_index(letter):\n",
        "    return torch.tensor(ALL_LETTERS.find(letter), dtype=torch.long )\n",
        "\n",
        "def word_to_indices(word,  n_vocab=NUM_LETTERS):\n",
        "    '''Returns a list of character indices for a given word'''\n",
        "    indices = []\n",
        "    for c in word:\n",
        "        indices.append(ALL_LETTERS.find(c))\n",
        "    return indices\n",
        "\n",
        "def text_transform(text, max_length=80, vocab_size=NUM_LETTERS):\n",
        "    '''Transform a string into a tensor with indices instead of one-hot encoding.'''\n",
        "    # Tokenizzazione: converti ogni lettera in un indice\n",
        "    indices = [ALL_LETTERS.find(char) for char in text]\n",
        "\n",
        "    # Padding o Troncamento per lunghezza fissa\n",
        "    if len(indices) < max_length:\n",
        "        indices += [0] * (max_length - len(indices))  # Pad con zeri (carattere vuoto)\n",
        "    else:\n",
        "        indices = indices[:max_length]  # Troncamento se il testo è più lungo\n",
        "\n",
        "    # Restituisci il tensore di indici\n",
        "    return torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "class CentralizedShakespeareDataset(Dataset):\n",
        "    def __init__(self, root, split, preprocess_params=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the dataset directory.\n",
        "            split (str): Dataset split, either 'train' or 'test'.\n",
        "            preprocess_params (dict, optional): Parameters for running preprocess.sh script. Keys include:\n",
        "                - sharding (str): 'iid' or 'niid' for data partitioning.\n",
        "                - iu (float): Fraction of users if i.i.d. sampling.\n",
        "                - sf (float): Fraction of data to sample.\n",
        "                - k (int): Minimum number of samples per user.\n",
        "                - t (str): 'user' or 'sample' for train-test partition.\n",
        "                - tf (float): Fraction of data in training set.\n",
        "                - raw (bool): Include raw text data.\n",
        "                - smplseed (int): Seed for sampling.\n",
        "                - spltseed (int): Seed for splitting.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.preprocess_params = preprocess_params or {}\n",
        "\n",
        "\n",
        "        # Ensure the working directory is set to the dataset folder\n",
        "        os.chdir(self.root)\n",
        "\n",
        "        # Run preprocessing script if needed\n",
        "        self._preprocess_data()\n",
        "\n",
        "        # Load the dataset\n",
        "        self.data = self._load_data()\n",
        "\n",
        "        # Create a label map to convert string targets to integers\n",
        "        #self.label_map = self.create_label_map()\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"Runs preprocess.sh with the given parameters.\"\"\"\n",
        "        cmd = \"bash preprocess.sh\"\n",
        "\n",
        "        if 'sharding' in self.preprocess_params:\n",
        "            cmd += f\" -s {self.preprocess_params['sharding']}\"\n",
        "        if 'iu' in self.preprocess_params:\n",
        "            cmd += f\" --iu {self.preprocess_params['iu']}\"\n",
        "        if 'sf' in self.preprocess_params:\n",
        "            cmd += f\" --sf {self.preprocess_params['sf']}\"\n",
        "        if 'k' in self.preprocess_params:\n",
        "            cmd += f\" -k {self.preprocess_params['k']}\"\n",
        "        if 't' in self.preprocess_params:\n",
        "            cmd += f\" -t {self.preprocess_params['t']}\"\n",
        "        if 'tf' in self.preprocess_params:\n",
        "            cmd += f\" --tf {self.preprocess_params['tf']}\"\n",
        "        if 'raw' in self.preprocess_params and self.preprocess_params['raw']:\n",
        "            cmd += f\" --raw\"\n",
        "        if 'smplseed' in self.preprocess_params:\n",
        "            cmd += f\" --smplseed {self.preprocess_params['smplseed']}\"\n",
        "        if 'spltseed' in self.preprocess_params:\n",
        "            cmd += f\" --spltseed {self.preprocess_params['spltseed']}\"\n",
        "\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        os.system(cmd)\n",
        "        os.chdir(self.root)\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"Loads data from the JSON file in the train or test folder, assuming only one file per folder.\"\"\"\n",
        "        folder_path = os.path.join(self.root, 'data', self.split)\n",
        "        print(f\"Absolute folder path: {os.path.abspath(folder_path)}\")\n",
        "        json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
        "\n",
        "        if len(json_files) != 1:\n",
        "            raise ValueError(f\"Expected exactly one JSON file in {folder_path}, but found {len(json_files)} files.\")\n",
        "\n",
        "        file_path = os.path.join(folder_path, json_files[0])\n",
        "\n",
        "        # Carica i dati dal file JSON\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Converti la struttura JSON in un DataFrame di pandas\n",
        "        records = []\n",
        "        for user, user_data in data['user_data'].items():\n",
        "            for x, y in zip(user_data['x'], user_data['y']):\n",
        "                records.append({\n",
        "                    'client_id': int(user),\n",
        "                    'x': x,  # Cambiato input in x\n",
        "                    'y': y   # Cambiato target in y\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "    def create_label_map(self):\n",
        "        \"\"\"Creates a mapping from string labels to integer labels.\"\"\"\n",
        "        unique_labels = sorted(self.data['y'].unique())\n",
        "        print(f\"Unique labels: {unique_labels}\")  # Debug\n",
        "        label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        return label_map\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        \"\"\"Returns the dataset as a pandas DataFrame.\"\"\"\n",
        "        return self.data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'x': self.data.iloc[idx]['x'],\n",
        "            'y': self.data.iloc[idx]['y']\n",
        "        }\n",
        "\n",
        "        sample['x'] = text_transform(sample['x'])  # x is a tensor of one-hot vectors\n",
        "        sample['y'] = text_transform(sample['y'])  # y is a tensor of one-hot vectors\n",
        "\n",
        "        return sample['x'], sample['y']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ASBPuONliim"
      },
      "source": [
        "## Shakespeare Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6JWBdeWlmBT"
      },
      "outputs": [],
      "source": [
        "class ShakespeareLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size=80, embed_dim=8, lstm_hidden_dim=256, seq_len=80, batch_size=32):\n",
        "        super(ShakespeareLSTM, self).__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "\n",
        "        # First LSTM layer\n",
        "        self.lstm1 = nn.LSTM(input_size=embed_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Second LSTM layer\n",
        "        self.lstm2 = nn.LSTM(input_size=lstm_hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Dense output layer\n",
        "        self.dense = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"\n",
        "        Inizializza lo stato nascosto e la cella della LSTM come tensori di zeri.\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(2, batch_size, self.lstm_hidden_dim).to(self.device)\n",
        "        c0 = torch.zeros(2, batch_size, self.lstm_hidden_dim).to(self.device)\n",
        "        return (h0, c0)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x: (batch_size, seq_len)\n",
        "        batch_size = x.size(0)\n",
        "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        x, hidden = self.lstm1(x, hidden)  # (batch_size, seq_len, lstm_hidden_dim)\n",
        "\n",
        "        x, hidden = self.lstm2(x, hidden)  # (batch_size, seq_len, lstm_hidden_dim)\n",
        "\n",
        "        x = self.dense(x)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return x, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPueHaDjcwN"
      },
      "source": [
        "## Centralized training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8YKXaBSjcwP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, validation_losses, validation_accuracies= [], [], []\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    start_epoch, json_data = load_checkpoint(model, optimizer, hyperparameters)\n",
        "    if json_data is not None:\n",
        "        validation_losses = json_data.get('validation_losses', [])\n",
        "        validation_accuracies = json_data.get('validation_accuracies', [])\n",
        "        train_losses = json_data.get('train_losses', [])\n",
        "\n",
        "    if start_epoch >= epochs:\n",
        "        print(f\"Checkpoint found, configuration already completed. Evaluating only on validation set.\")\n",
        "        validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "        return train_losses, validation_losses, validation_accuracies\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(start_epoch, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        total_batches = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            try:\n",
        "                # Ensure inputs and targets are within valid range\n",
        "                if inputs.max() >= model.vocab_size or inputs.min() < 0:\n",
        "                    print(f\"Invalid input indices found. Max: {inputs.max()}, Min: {inputs.min()}\")\n",
        "                    continue\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs, _ = model(inputs)  # Ignora gli hidden states\n",
        "\n",
        "\n",
        "                # Reshape for loss calculation\n",
        "                batch_size, seq_len, vocab_size = outputs.shape\n",
        "                outputs = outputs.view(-1, vocab_size)\n",
        "                targets = targets.view(-1)\n",
        "\n",
        "                # Create mask for non-padding tokens\n",
        "                non_pad_mask = targets != 0\n",
        "\n",
        "                # Only compute loss on non-padding tokens\n",
        "                valid_outputs = outputs[non_pad_mask]\n",
        "                valid_targets = targets[non_pad_mask]\n",
        "\n",
        "                if len(valid_targets) > 0:  # Only compute loss if we have valid tokens\n",
        "                    loss = criterion(valid_outputs, valid_targets)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_loss += loss.item()\n",
        "                    total_batches += 1\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        if total_batches > 0:\n",
        "            epoch_loss = epoch_loss / total_batches\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss)\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(\n",
        "            model, optimizer, epoch, hyperparameters, \"Centralized/\",\n",
        "            data_to_save={\n",
        "                'validation_losses': validation_losses,\n",
        "                'validation_accuracies': validation_accuracies,\n",
        "                'train_losses': train_losses\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, validation_losses, validation_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            try:\n",
        "                if inputs.max() >= model.vocab_size or inputs.min() < 0:\n",
        "                    continue\n",
        "\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                outputs,_ = model(inputs)\n",
        "                batch_size, seq_len, vocab_size = outputs.shape\n",
        "                outputs = outputs.view(-1, vocab_size)\n",
        "                targets = targets.view(-1)\n",
        "\n",
        "                # Create mask for non-padding tokens\n",
        "                non_pad_mask = targets != 0\n",
        "                valid_outputs = outputs[non_pad_mask]\n",
        "                valid_targets = targets[non_pad_mask]\n",
        "\n",
        "                if len(valid_targets) > 0:\n",
        "                    loss = criterion(valid_outputs, valid_targets)\n",
        "                    total_loss += loss.item()\n",
        "                    total_batches += 1\n",
        "\n",
        "                    _, predicted = valid_outputs.max(1)\n",
        "                    total += valid_targets.size(0)\n",
        "                    correct += (predicted == valid_targets).sum().item()\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error during evaluation: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    avg_loss = total_loss / total_batches if total_batches > 0 else float('inf')\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rys0x0RL4xDn"
      },
      "source": [
        "## Centralized training Experimetns\n",
        "Run the cells below before starting any experiments, to regenerate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qfjvLkl-_P3",
        "outputId": "8ba573cf-f912-45fe-b414-8a176614635f"
      },
      "outputs": [],
      "source": [
        "# Always run before creating new datasets\n",
        "\n",
        "if os.path.exists(\"/content/leaf/\"):\n",
        "  # Use shutil.rmtree to remove the folder and its contents\n",
        "  shutil.rmtree(\"/content/leaf\")\n",
        "  print(f\"Successfully deleted folder leaf\")\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/maxfra01/leaf.git\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "preprocess_params = {\n",
        "        'sharding': 'iid',\n",
        "        'sf': 0.01,\n",
        "        'iu': 0.089,\n",
        "        't': 'sample',\n",
        "        'tf': 0.8,\n",
        "    } # Get the full-size dataset\n",
        "\n",
        "train_dataset_big = CentralizedShakespeareDataset(root=\"/content/leaf/data/shakespeare\", split=\"train\", preprocess_params=preprocess_params)\n",
        "test_dataset = CentralizedShakespeareDataset(root=\"/content/leaf/data/shakespeare\", split=\"test\", preprocess_params=preprocess_params)\n",
        "print(f\"Train dataset: {len(train_dataset_big)} samples\\n\"\n",
        "      f\"Test dataset: {len(test_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywbly008ysvw"
      },
      "source": [
        "# Centralized experiments\n",
        "Here hyperparameters can be changed. Run this cell to start experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bUGr07HXneev",
        "outputId": "b3b04ef3-1203-4117-cc71-ad1472c3af3c"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = [0.01, 0.005, 0.001]\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY= [1e-3, 1e-4, 1e-5]\n",
        "EPOCHS = 20\n",
        "\n",
        "hyperparameters = f\"BS{BATCH_SIZE}_LR{LEARNING_RATE}_WD{WEIGHT_DECAY}_M{MOMENTUM}\"\n",
        "\n",
        "model_shakespeare = ShakespeareLSTM()\n",
        "\n",
        "indexes = range(0, len(train_dataset_big))\n",
        "splitting = train_test_split(indexes, train_size=0.8, random_state=42, shuffle=True)\n",
        "train_indexes = splitting[0]\n",
        "val_indexes = splitting[1]\n",
        "\n",
        "train_dataset = Subset(train_dataset_big, train_indexes)\n",
        "val_dataset = Subset(train_dataset_big, val_indexes)\n",
        "\n",
        "\n",
        "# Create Dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=2)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=2)\n",
        "\n",
        "# Variabile per salvare i migliori risultati\n",
        "best_result = {\n",
        "    \"hyperparameters\": None,\n",
        "    \"val_accuracy\": 0.0,\n",
        "    \"val_loss\": float('inf'),\n",
        "}\n",
        "\n",
        "# Ciclo su tutte le combinazioni di iperparametri\n",
        "for lr in LEARNING_RATE:\n",
        "    for wd in WEIGHT_DECAY:\n",
        "        print(f\"\\nTesting with LR={lr} and WD={wd}\")\n",
        "\n",
        "        # Inizializzazione del modello e degli ottimizzatori\n",
        "        model_shakespeare = ShakespeareLSTM()\n",
        "        optimizer = optim.SGD(\n",
        "            model_shakespeare.parameters(),\n",
        "            lr=lr,\n",
        "            momentum=MOMENTUM,\n",
        "            weight_decay=wd\n",
        "        )\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Allenamento del modello\n",
        "        train_losses, val_losses, val_accuracies = train_model(\n",
        "            model=model_shakespeare,\n",
        "            train_loader=train_dataloader,\n",
        "            validation_loader=val_dataloader,\n",
        "            test_loader=test_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            criterion=criterion,\n",
        "            epochs=EPOCHS,\n",
        "            hyperparameters=f\"BS{BATCH_SIZE}_LR{lr}_WD{wd}_M{MOMENTUM}\"\n",
        "        )\n",
        "\n",
        "        # Registra i migliori risultati\n",
        "        final_val_accuracy = val_accuracies[-1]\n",
        "        final_val_loss = val_losses[-1]\n",
        "        if final_val_accuracy > best_result[\"val_accuracy\"]:\n",
        "            best_result[\"hyperparameters\"] = f\"LR={lr}, WD={wd}\"\n",
        "            best_result[\"val_accuracy\"] = final_val_accuracy\n",
        "            best_result[\"val_loss\"] = final_val_loss\n",
        "            print(f\"New best result -> Val Accuracy: {final_val_accuracy:.4f}, Val Loss: {final_val_loss:.4f}\")\n",
        "\n",
        "        # Grafici per ogni combinazione\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Validation Loss\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(val_losses, label=f'Val Loss (LR={lr}, WD={wd})')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.savefig('validation_loss.png')\n",
        "\n",
        "        # Validation Accuracy\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(val_accuracies, label=f'Val Accuracy (LR={lr}, WD={wd})')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.savefig('validation_accuracy.png')\n",
        "\n",
        "        plt.suptitle(f\"Results for LR={lr}, WD={wd}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Stampa dei migliori iperparametri\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(f\"{best_result['hyperparameters']} -> Val Accuracy: {best_result['val_accuracy']:.4f}, Val Loss: {best_result['val_loss']:.4f}, Test Accuracy: {best_result['test_accuracy']:.4f}, Test Loss: {best_result['test_loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwjMj8-N0K7s"
      },
      "source": [
        "## Federated Learning classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyQY3IBN0NXE"
      },
      "outputs": [],
      "source": [
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "\n",
        "  def train(self, global_weights, local_steps, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
        "\n",
        "    steps = 0\n",
        "    while steps < local_steps:\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = self.model(inputs)  # Ignora gli hidden states\n",
        "\n",
        "        batch_size, seq_len, vocab_size = outputs.shape\n",
        "        outputs = outputs.view(-1, vocab_size)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        non_pad_mask = targets != 0\n",
        "\n",
        "        # Only compute loss on non-padding tokens\n",
        "        valid_outputs = outputs[non_pad_mask]\n",
        "        valid_targets = targets[non_pad_mask]\n",
        "\n",
        "        if len(valid_targets) > 0:  # Only compute loss if we have valid tokens\n",
        "          loss = criterion(valid_outputs, valid_targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          steps += 1\n",
        "        if steps >= local_steps:\n",
        "          break\n",
        "\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n",
        "\n",
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data, val_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.val_data = val_data\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "    self.selected_clients_per_round = [] #clint selezionati per skewness\n",
        "\n",
        "  def federated_averaging(self, local_steps, batch_size, num_rounds, fraction_fit, hyperparameters = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "\n",
        "    # Carica il checkpoint se esiste\n",
        "    data_to_load = None\n",
        "    start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "\n",
        "    if data_to_load is not None:\n",
        "      self.round_losses = data_to_load['round_losses']\n",
        "      self.round_accuracies = data_to_load['round_accuracies']\n",
        "      self.selected_clients_per_round = data_to_load['selected_clients_per_round']\n",
        "\n",
        "\n",
        "    for round in range(start_epoch, num_rounds+1):\n",
        "\n",
        "      selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False)\n",
        "\n",
        "      self.selected_clients_per_round.append([client.client_id for client in selected_clients])\n",
        "\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, local_steps, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model every rounds\n",
        "      if round % 1 == 0:\n",
        "        loss, accuracy = evaluate_model(self.model, DataLoader(self.val_data, batch_size=batch_size, shuffle=False, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "\n",
        "        self.round_losses.append(loss)\n",
        "        self.round_accuracies.append(accuracy)\n",
        "        print(f\"Round {round}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        data_to_save = {\n",
        "          'round_losses': self.round_losses,\n",
        "          'round_accuracies': self.round_accuracies,\n",
        "          'selected_clients_per_round': [[client for client in round_clients] for round_clients in self.selected_clients_per_round]  # Serializziamo solo i client_id\n",
        "      }\n",
        "\n",
        "        save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\", data_to_save)\n",
        "\n",
        "    print(\"Evaluation on test set...\")\n",
        "    loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=False, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     # Plot dei risultati\n",
        "    plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Validation Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(range(0, num_rounds, 10), self.round_losses, label='Validation Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss per Round')\n",
        "    plt.legend()\n",
        "\n",
        "        # Validation Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(range(0, num_rounds, 10), self.round_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Accuracy per Round')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    file_name = f\"Shakespeare_fedavg_uniform_{hyperparameters}.jpg\"\n",
        "    plt.savefig(file_name)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oAwEaxgNkEe"
      },
      "source": [
        "# Federated Learning Experiments\n",
        "Run all cell below. Hyperparameters can be edited in the very last cell. Run this cell before starting any experimeents, to regenerate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTRGNn2F1zp7",
        "outputId": "6d886d32-023e-4cc3-b6ae-3cf3c2a9bd6b"
      },
      "outputs": [],
      "source": [
        "# Always run before creating new datasets\n",
        "\n",
        "if os.path.exists(\"/content/leaf/\"):\n",
        "  # Use shutil.rmtree to remove the folder and its contents\n",
        "  shutil.rmtree(\"/content/leaf\")\n",
        "  print(f\"Successfully deleted folder leaf\")\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/maxfra01/leaf.git\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "preprocess_params = {\n",
        "        'sharding': 'iid',\n",
        "        'sf': 0.06,\n",
        "        'iu': 0.089,\n",
        "        't': 'sample',\n",
        "        'tf': 0.8,\n",
        "    } # Get the full-size dataset\n",
        "\n",
        "train_dataset_big = CentralizedShakespeareDataset(root=\"/content/leaf/data/shakespeare\", split=\"train\", preprocess_params=preprocess_params)\n",
        "test_dataset = CentralizedShakespeareDataset(root=\"/content/leaf/data/shakespeare\", split=\"test\", preprocess_params=preprocess_params)\n",
        "print(f\"Train dataset: {len(train_dataset_big)} samples\\n\"\n",
        "      f\"Test dataset: {len(test_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP7ljUvOyMm9"
      },
      "source": [
        "# RUN FedAVG experiments\n",
        "\n",
        "Hyperparameters can be changed here. Run this cell to start the experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU3l7j9I17Fa",
        "outputId": "ded84aaa-c24a-41e7-db20-8d2be5c64c92"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "LR = 10**(-0.5)\n",
        "WEIGHT_DECAY = 1e-4\n",
        "MOMENTUM = 0\n",
        "ROUNDS = None\n",
        "LOCAL_STEPS = None\n",
        "C = 0.1\n",
        "K=100\n",
        "\n",
        "optimizer_params = {\n",
        "      \"lr\": LR,\n",
        "      \"momentum\": MOMENTUM,\n",
        "      \"weight_decay\": WEIGHT_DECAY\n",
        "  }\n",
        "\n",
        "for ROUNDS, LOCAL_STEPS in [(200,4), (100, 8), (50, 16)]:\n",
        "  hyperparameters = hyperparameters = f\"BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_STEPS}_C{C}\"\n",
        "\n",
        "  model_shakespeare = ShakespeareLSTM()\n",
        "\n",
        "  train_indices, validation_indices = train_test_split(\n",
        "    range(len(train_dataset_big)), test_size=0.2, random_state=42\n",
        "  )\n",
        "\n",
        "  train_dataset = Subset(train_dataset_big, train_indices)\n",
        "  validation_dataset = Subset(train_dataset_big, validation_indices)\n",
        "\n",
        "  original_to_subset = {original_idx: subset_idx for subset_idx, original_idx in enumerate(train_indices)}\n",
        "\n",
        "  clients = []\n",
        "  for i in range(K):\n",
        "      client_original_indices = train_dataset_big.data[\n",
        "          train_dataset_big.data[\"client_id\"] == i\n",
        "      ].index\n",
        "\n",
        "      # Converte gli indici originali in indici del subset\n",
        "      client_subset_indices = [original_to_subset[idx] for idx in client_original_indices if idx in original_to_subset]\n",
        "\n",
        "      # Crea il subset per il client\n",
        "      client_data = Subset(train_dataset, client_subset_indices)\n",
        "      clients.append(Client(model_shakespeare, i, client_data, optimizer_params))\n",
        "\n",
        "\n",
        "\n",
        "  server_uniform = Server(model_shakespeare, clients, test_dataset, validation_dataset)\n",
        "  server_uniform.federated_averaging(local_steps=LOCAL_STEPS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C,hyperparameters=hyperparameters)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
