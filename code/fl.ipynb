{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK5E-uqb92ci"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import drive\n",
        "\n",
        "# Set the working directory\n",
        "DIR_DATA = '/content/'\n",
        "os.chdir(DIR_DATA)\n"
      ],
      "metadata": {
        "id": "LVs00uLvecJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR100 Dataset"
      ],
      "metadata": {
        "id": "GBqh7noxfUWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR100Dataset(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None, sharding=None, K=10, Nc=2):\n",
        "        \"\"\"\n",
        "        CIFAR-100 Dataset with IID and non-IID sharding.\n",
        "\n",
        "        Args:\n",
        "        - root (str): Directory to store the dataset.\n",
        "        - split (str): 'train' or 'test'.\n",
        "        - transform (callable): Transformations applied to the images.\n",
        "        - sharding (str): 'iid' or 'niid'.\n",
        "        - K (int): Number of clients for the sharding.\n",
        "        - Nc (int): Number of classes per client (used for non-iid sharding).\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.sharding = sharding\n",
        "        self.K = K\n",
        "        self.Nc = Nc\n",
        "\n",
        "        # Default transformations if none are provided\n",
        "        if self.transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "              transforms.Resize((224, 224)),  # Resize CIFAR-100 (32x32) to ResNet's input size (224x224)\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # CIFAR-100 normalization\n",
        "        ])\n",
        "\n",
        "        dataset = datasets.CIFAR100(\n",
        "            root=self.root,\n",
        "            train=(self.split == 'train'),\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.data = pd.DataFrame({\n",
        "            \"image\": [dataset[i][0] for i in range(len(dataset))],\n",
        "            \"label\": [dataset[i][1] for i in range(len(dataset))]\n",
        "        })\n",
        "\n",
        "        if self.split == 'train' and self.sharding:\n",
        "            self.data = self._apply_sharding()\n",
        "\n",
        "    def _apply_sharding(self):\n",
        "        \"\"\"Apply IID or non-IID sharding to the training data.\"\"\"\n",
        "        if self.sharding == 'iid':\n",
        "            return self._iid_sharding()\n",
        "        elif self.sharding == 'niid':\n",
        "            return self._non_iid_sharding()\n",
        "        else:\n",
        "            raise ValueError(\"Sharding must be 'iid' or 'niid'.\")\n",
        "\n",
        "    def _iid_sharding(self):\n",
        "        \"\"\"Split data IID: uniformly distribute samples across K clients.\"\"\"\n",
        "        data_split = []\n",
        "        indices = self.data.index.tolist()\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        # Split indices equally among K clients\n",
        "        client_indices = [indices[i::self.K] for i in range(self.K)]\n",
        "\n",
        "        for client_id, idxs in enumerate(client_indices):\n",
        "            client_data = self.data.loc[idxs].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def _non_iid_sharding(self):\n",
        "        \"\"\"Split data non-IID: assign Nc classes per client.\"\"\"\n",
        "        data_split = []\n",
        "        unique_classes = self.data['label'].unique()\n",
        "        random.shuffle(unique_classes)\n",
        "\n",
        "        # Divide classes into groups of Nc\n",
        "        class_groups = [unique_classes[i:i + self.Nc] for i in range(0, len(unique_classes), self.Nc)]\n",
        "        class_groups = class_groups[:self.K]  # Limit to K clients\n",
        "\n",
        "        for client_id, class_group in enumerate(class_groups):\n",
        "            client_data = self.data[self.data['label'].isin(class_group)].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "\n",
        "            # Ensure approximately equal samples per client\n",
        "            client_data = client_data.sample(n=len(self.data) // self.K, replace=True, random_state=42)\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        image, label = row['image'], row['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "Zbl-Cn0aw2CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.5071, 0.4867, 0.4408],\n",
        "        std=[0.2675, 0.2565, 0.2761]\n",
        "    )\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.5071, 0.4867, 0.4408],\n",
        "        std=[0.2675, 0.2565, 0.2761]\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "IyqkvB_vI2VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Centralized training of CIFAR100"
      ],
      "metadata": {
        "id": "YdRUiDbHz-ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler\n",
        "        if scheduler is not None:\n",
        "          scheduler.step()\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, test_losses, test_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n"
      ],
      "metadata": {
        "id": "RHgLtGILz8uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 96\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-3\n",
        "EPOCHS = 50\n",
        "\n",
        "train_dataset_big = CIFAR100Dataset(DIR_DATA, split='train')\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "\n",
        "indexes = range(0, len(train_dataset_big))\n",
        "splitting = train_test_split(indexes, train_size = 0.8, random_state = 42, stratify = train_dataset_big.data[\"label\"], shuffle = True)\n",
        "train_indexes = splitting[0]\n",
        "val_indexes = splitting[1]\n",
        "\n",
        "train_dataset = Subset(train_dataset_big, train_indexes)\n",
        "val_dataset = Subset(train_dataset_big, val_indexes)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model_cifar = models.resnet18(pretrained=True)\n",
        "\n",
        "for param in model_cifar.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_cifar.fc = nn.Linear(model_cifar.fc.in_features, 100)\n",
        "\n",
        "for param in model_cifar.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer_cifar = optim.SGD(model_cifar.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler_cifar = CosineAnnealingLR(optimizer_cifar, T_max=200)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_losses_cifar, test_losses_cifar, test_accuracies_cifar = train_model(\n",
        "    model=model_cifar,\n",
        "    train_loader = train_dataloader,\n",
        "    test_loader = test_dataloader,\n",
        "    optimizer=optimizer_cifar,\n",
        "    scheduler=scheduler_cifar,\n",
        "    criterion=criterion,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(test_losses_cifar, label='CIFAR-100 Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_accuracies_cifar, label='CIFAR-100 Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m1JKWoaq7ODR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FL Baseline"
      ],
      "metadata": {
        "id": "8oCdLhW1Vv80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, epochs, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "      print(f\"Client {self.client_id}, Epoch {epoch+1}/{epochs}\")\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bxpBSJyCoSPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "\n",
        "  def federated_averaging(self, epochs, batch_size, num_rounds, fraction_fit, skewness = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    for round in range(num_rounds):\n",
        "      print(f\"Round {round+1}/{num_rounds}\")\n",
        "\n",
        "      if skewness is not None:\n",
        "        probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False, p=probabilities)\n",
        "\n",
        "      else:\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False)\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, epochs, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model\n",
        "      loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True), nn.CrossEntropyLoss(), device)\n",
        "      self.round_losses.append(loss)\n",
        "      self.round_accuracies.append(accuracy)\n",
        "      print(f\"Round {round+1}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='CIFAR-100 Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='CIFAR-100 Test Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "KkSMDm61Acaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = 100\n",
        "LOCAL_EPOCHS = 4 # J\n",
        "ROUNDS = 2000\n",
        "C = 0.1\n",
        "BATCH_SIZE = 16\n",
        "LR = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-3\n",
        "\n",
        "optimizer_params = {\n",
        "    \"lr\": LR,\n",
        "    \"momentum\": MOMENTUM,\n",
        "    \"weight_decay\": WEIGHT_DECAY\n",
        "}\n",
        "\n",
        "model_cifar = models.resnet18(pretrained=True)\n",
        "model_cifar.fc = nn.Linear(model_cifar.fc.in_features, 100)\n",
        "\n",
        "train_dataset = CIFAR100Dataset(DIR_DATA, split='train', sharding='iid', K=K)\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "  client_data = Subset(train_dataset, train_dataset.data[train_dataset.data[\"client_id\"] == i].index)\n",
        "  clients.append(Client(model_cifar, i, client_data, optimizer_params))\n",
        "\n",
        "server_uniform = Server(model_cifar, clients, test_dataset)\n",
        "\n",
        "server_uniform.federated_averaging(epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C)\n",
        "\n",
        "#server_skewed = Server(model_cifar, clients, test_dataset)\n",
        "\n",
        "#server_skewed.federated_averaging(epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C, skewness=0.5)"
      ],
      "metadata": {
        "id": "nEh36xySq9Pr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}