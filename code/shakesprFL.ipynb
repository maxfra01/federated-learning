{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Third-party library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Google Colab specific imports\n",
    "from google.colab import drive\n",
    "\n",
    "# Set the working directory\n",
    "DIR_DATA = '/content/'\n",
    "os.chdir(DIR_DATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses, test_losses, test_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Step the scheduler\n",
    "        if scheduler is not None:\n",
    "          scheduler.step()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
    "        train_losses.append(epoch_loss / len(train_loader))\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses, test_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    return total_loss / len(test_loader), correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federate Learning classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skewed_probabilities(num_clients, gamma):\n",
    "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
    "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "class Client:\n",
    "\n",
    "  def __init__(self, model, client_id, data, optimizer_params):\n",
    "    self.client_id = client_id\n",
    "    self.data = data\n",
    "    self.model = model\n",
    "    self.optimizer_params = optimizer_params\n",
    "\n",
    "  def train(self, global_weights, epochs, batch_size):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.model.to(device)\n",
    "    self.model.load_state_dict(global_weights)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(\n",
    "        self.model.parameters(),\n",
    "        lr=self.optimizer_params['lr'],\n",
    "        momentum=self.optimizer_params['momentum'],\n",
    "        weight_decay=self.optimizer_params['weight_decay']\n",
    "        )\n",
    "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "      print(f\"Client {self.client_id}, Epoch {epoch+1}/{epochs}\")\n",
    "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = self.model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return self.model.state_dict()\n",
    "\n",
    "\n",
    "\n",
    "class Server:\n",
    "  def __init__(self, model, clients, test_data):\n",
    "    self.model = model\n",
    "    self.clients = clients\n",
    "    self.test_data = test_data\n",
    "    self.round_losses = []\n",
    "    self.round_accuracies = []\n",
    "\n",
    "  def federated_averaging(self, epochs, batch_size, num_rounds, fraction_fit, skewness=None, fedOptimizer=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    self.model.to(device)\n",
    "\n",
    "    # Initialize variables for FedOptimizers\n",
    "    if fedOptimizer in {\"FedAdaGrad\", \"FedYogi\", \"FedAdam\"}:\n",
    "        optimizer_state = {\n",
    "            \"m\": {key: torch.zeros_like(value, dtype=torch.float32) for key, value in self.model.state_dict().items()},\n",
    "            \"v\": {key: torch.zeros_like(value, dtype=torch.float32) for key, value in self.model.state_dict().items()},\n",
    "        }\n",
    "        beta1 = 0.9  # Momentum parameter for Adam-based optimizers\n",
    "        beta2 = 0.999  # RMS parameter for Adam-based optimizers\n",
    "        lr = 0.01  # Learning rate\n",
    "        eps = 1e-8  # Small constant to prevent division by zero\n",
    "\n",
    "    for round in range(num_rounds):\n",
    "        print(f\"Round {round + 1}/{num_rounds}\")\n",
    "\n",
    "        if skewness is not None:\n",
    "            probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
    "            selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit * len(self.clients))),\n",
    "                                                replace=False, p=probabilities)\n",
    "        else:\n",
    "            selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit * len(self.clients))),\n",
    "                                                replace=False)\n",
    "\n",
    "        global_weights = self.model.state_dict()\n",
    "\n",
    "        # Simulate parallel client training\n",
    "        client_weights = {}\n",
    "        for client in selected_clients:\n",
    "            client_weights[client.client_id] = client.train(global_weights, epochs, batch_size)\n",
    "\n",
    "        # Aggregate client updates\n",
    "        total_data_size = sum([len(client.data) for client in selected_clients])\n",
    "        aggregated_updates = {key: torch.zeros_like(value, dtype=torch.float32) for key, value in global_weights.items()}\n",
    "\n",
    "        for client in selected_clients:\n",
    "            scaling_factor = len(client.data) / total_data_size\n",
    "            for key in aggregated_updates.keys():\n",
    "                aggregated_updates[key] += scaling_factor * (client_weights[client.client_id][key] - global_weights[key])\n",
    "\n",
    "        # Apply selected FedOptimizer\n",
    "        if fedOptimizer == \"FedAdaGrad\":\n",
    "            for key in global_weights.keys():\n",
    "                optimizer_state[\"v\"][key] += aggregated_updates[key] ** 2\n",
    "                global_weights[key] += lr * aggregated_updates[key] / (torch.sqrt(optimizer_state[\"v\"][key]) + eps)\n",
    "\n",
    "        elif fedOptimizer == \"FedYogi\":\n",
    "            for key in global_weights.keys():\n",
    "                optimizer_state[\"v\"][key] -= (1 - beta2) * aggregated_updates[key] ** 2 * torch.sign(\n",
    "                    optimizer_state[\"v\"][key] - aggregated_updates[key] ** 2)\n",
    "                global_weights[key] += lr * aggregated_updates[key] / (torch.sqrt(optimizer_state[\"v\"][key]) + eps)\n",
    "\n",
    "        elif fedOptimizer == \"FedAdam\":\n",
    "            for key in global_weights.keys():\n",
    "                optimizer_state[\"m\"][key] = beta1 * optimizer_state[\"m\"][key] + (1 - beta1) * aggregated_updates[key]\n",
    "                optimizer_state[\"v\"][key] = beta2 * optimizer_state[\"v\"][key] + (1 - beta2) * aggregated_updates[key] ** 2\n",
    "                m_hat = optimizer_state[\"m\"][key] / (1 - beta1 ** (round + 1))\n",
    "                v_hat = optimizer_state[\"v\"][key] / (1 - beta2 ** (round + 1))\n",
    "                global_weights[key] += lr * m_hat / (torch.sqrt(v_hat) + eps)\n",
    "\n",
    "        else:  # Default to FedAvg\n",
    "            for key in global_weights.keys():\n",
    "                global_weights[key] += aggregated_updates[key]\n",
    "\n",
    "        # Update global model weights\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "        # Evaluate global model\n",
    "        loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True),\n",
    "                                        nn.CrossEntropyLoss(), device)\n",
    "        self.round_losses.append(loss)\n",
    "        self.round_accuracies.append(accuracy)\n",
    "        print(f\"Round {round + 1}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(self.round_losses, label='Test Loss')\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(self.round_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Round')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
