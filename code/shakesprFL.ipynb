{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbHNmOvzjcwC"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GTbCMkk8jcwK"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import drive\n",
        "\n",
        "# Set the working directory\n",
        "DIR_DATA = '/content/'\n",
        "os.chdir(DIR_DATA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "id": "2d0FdWHolo_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a Folder in the root directory\n",
        "!mkdir -p \"/content/drive/My Drive/My Folder/checkpoints_shakespeare\"\n",
        "\n",
        "DIR_DATA = \"./data\"\n",
        "CHECKPOINT_DIR = '/content/drive/My Drive/My Folder/checkpoints_shakespeare'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", data_to_save=None):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -1}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "    previous_filename_json = f\"model_epoch_{epoch -1}_params_{hyperparameters}.json\"\n",
        "    previous_filepath_json = os.path.join(subfolder_path, previous_filename_json)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath) and os.path.exists(previous_filepath_json):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "    with open(filepath_json, 'w') as json_file:\n",
        "      json.dump(data_to_save, json_file, indent=4)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1, None  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca pi√π alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Trova e carica il file JSON associato\n",
        "        json_filename = latest_file.replace('.pth', '.json')\n",
        "        json_filepath = os.path.join(subfolder_path, json_filename)\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(f\"JSON data loaded: {json_filepath}\")\n",
        "        else:\n",
        "            print(f\"No JSON file found for: {latest_file}\")\n",
        "\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1, None  # Le epoche iniziano da 1\n",
        "\n"
      ],
      "metadata": {
        "id": "vVQ8Xif5lqil",
        "outputId": "501332bf-e467-4a4b-b2cb-310c583a2aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Dataset"
      ],
      "metadata": {
        "id": "j3i5KwH-pF1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_transform(text, max_length=100, vocab_size=100):\n",
        "    # Tokenizzazione: converti ogni carattere in un valore numerico (es. ASCII)\n",
        "    tokenized = [ord(char) % vocab_size for char in text]\n",
        "\n",
        "    # Padding o Troncamento per lunghezza fissa\n",
        "    if len(tokenized) < max_length:\n",
        "        tokenized += [0] * (max_length - len(tokenized))  # Pad con zeri\n",
        "    else:\n",
        "        tokenized = tokenized[:max_length]  # Troncamento\n",
        "\n",
        "    # Converte in tensore di tipo LongTensor\n",
        "    return torch.tensor(tokenized, dtype=torch.long)\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, root, split, preprocess_params=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the dataset directory.\n",
        "            split (str): Dataset split, either 'train' or 'test'.\n",
        "            preprocess_params (dict, optional): Parameters for running preprocess.sh script. Keys include:\n",
        "                - sharding (str): 'iid' or 'niid' for data partitioning.\n",
        "                - iu (float): Fraction of users if i.i.d. sampling.\n",
        "                - sf (float): Fraction of data to sample.\n",
        "                - k (int): Minimum number of samples per user.\n",
        "                - t (str): 'user' or 'sample' for train-test partition.\n",
        "                - tf (float): Fraction of data in training set.\n",
        "                - raw (bool): Include raw text data.\n",
        "                - smplseed (int): Seed for sampling.\n",
        "                - spltseed (int): Seed for splitting.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.preprocess_params = preprocess_params or {}\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = text_transform\n",
        "\n",
        "        # Ensure the working directory is set to the dataset folder\n",
        "        os.chdir(self.root)\n",
        "\n",
        "        # Run preprocessing script if needed\n",
        "        self._preprocess_data()\n",
        "\n",
        "        # Load the dataset\n",
        "        self.data = self._load_data()\n",
        "\n",
        "        # Create a label map to convert string targets to integers\n",
        "        self.label_map = self.create_label_map()\n",
        "\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"Runs preprocess.sh with the given parameters.\"\"\"\n",
        "        cmd = \"bash preprocess.sh\"\n",
        "\n",
        "        if 'sharding' in self.preprocess_params:\n",
        "            cmd += f\" -s {self.preprocess_params['sharding']}\"\n",
        "        if 'iu' in self.preprocess_params:\n",
        "            cmd += f\" --iu {self.preprocess_params['iu']}\"\n",
        "        if 'sf' in self.preprocess_params:\n",
        "            cmd += f\" --sf {self.preprocess_params['sf']}\"\n",
        "        if 'k' in self.preprocess_params:\n",
        "            cmd += f\" -k {self.preprocess_params['k']}\"\n",
        "        if 't' in self.preprocess_params:\n",
        "            cmd += f\" -t {self.preprocess_params['t']}\"\n",
        "        if 'tf' in self.preprocess_params:\n",
        "            cmd += f\" --tf {self.preprocess_params['tf']}\"\n",
        "        if 'raw' in self.preprocess_params and self.preprocess_params['raw']:\n",
        "            cmd += f\" --raw\"\n",
        "        if 'smplseed' in self.preprocess_params:\n",
        "            cmd += f\" --smplseed {self.preprocess_params['smplseed']}\"\n",
        "        if 'spltseed' in self.preprocess_params:\n",
        "            cmd += f\" --spltseed {self.preprocess_params['spltseed']}\"\n",
        "\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        os.system(cmd)\n",
        "        os.chdir(DIR_DATA)\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "      \"\"\"Loads data from the JSON file in the train or test folder, assuming only one file per folder.\"\"\"\n",
        "      # Identifica il file JSON nella directory specificata\n",
        "      folder_path = os.path.join(self.root,'data', self.split)\n",
        "      print(f\"Absolute folder path: {os.path.abspath(folder_path)}\")\n",
        "      json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
        "\n",
        "      if len(json_files) != 1:\n",
        "          raise ValueError(f\"Expected exactly one JSON file in {folder_path}, but found {len(json_files)} files.\")\n",
        "\n",
        "      file_path = os.path.join(folder_path, json_files[0])\n",
        "\n",
        "      # Carica i dati dal file JSON\n",
        "      with open(file_path, 'r') as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      # Converti la struttura JSON in un DataFrame di pandas\n",
        "      records = []\n",
        "      for user, user_data in data['user_data'].items():\n",
        "          for x, y in zip(user_data['x'], user_data['y']):\n",
        "              records.append({\n",
        "                  'user': user,\n",
        "                  'input': x,\n",
        "                  'target': y\n",
        "              })\n",
        "\n",
        "      return pd.DataFrame(records)\n",
        "\n",
        "    def create_label_map(self):\n",
        "      \"\"\"Creates a mapping from string labels to integer labels.\"\"\"\n",
        "      unique_labels = sorted(self.data['target'].unique())\n",
        "      print(f\"Unique labels: {unique_labels}\")  # Debug\n",
        "      label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "      return label_map\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        \"\"\"Returns the dataset as a pandas DataFrame.\"\"\"\n",
        "        return self.data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      sample = {\n",
        "          'input': self.data.iloc[idx]['input'],\n",
        "          'target': self.data.iloc[idx]['target']\n",
        "      }\n",
        "\n",
        "      # Applica la trasformazione agli input (e.g., tokenizzazione e padding)\n",
        "      if self.transform:\n",
        "          sample['input'] = self.transform(sample['input'])\n",
        "\n",
        "      # Verifica se il target √® nella mappatura\n",
        "      if sample['target'] not in self.label_map:\n",
        "          raise ValueError(f\"Unexpected target value: {sample['target']}\")\n",
        "\n",
        "      # Converte il target da stringa a indice numerico tramite la mappatura\n",
        "      sample['target'] = self.label_map[sample['target']]\n",
        "\n",
        "      # Converte il target in tensore\n",
        "      sample['target'] = torch.tensor(sample['target'], dtype=torch.long)\n",
        "\n",
        "      return sample['input'], sample['target']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KNrpQePKpJOS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Model Architecture"
      ],
      "metadata": {
        "id": "5ASBPuONliim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers):\n",
        "        super(ShakespeareRNN, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "    # Hidden state (h_0) e cell state (c_0) inizializzati a zero\n",
        "      return (\n",
        "          torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),\n",
        "          torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
        "      )\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)  # Embed input\n",
        "        out, hidden = self.lstm(x, hidden)  # Pass through LSTM layers\n",
        "        out = self.fc(out)  # Fully connected layer for output\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "e6JWBdeWlmBT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPueHaDjcwN"
      },
      "source": [
        "## Centralized training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S8YKXaBSjcwP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, validation_losses, validation_accuracies = [], [], []\n",
        "\n",
        "    # Carica checkpoint se esiste\n",
        "    start_epoch, json_data = load_checkpoint(model, optimizer, hyperparameters, \"Centralized/\")\n",
        "    if json_data is not None:\n",
        "        validation_losses = json_data.get('validation_losses', [])\n",
        "        validation_accuracies = json_data.get('validation_accuracies', [])\n",
        "        train_losses = json_data.get('train_losses', [])\n",
        "\n",
        "    if start_epoch >= epochs:\n",
        "        print(f\"Checkpoint trovato, configurazione gi√† completata. Valutazione solo sul validation set.\")\n",
        "        validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "        return train_losses, validation_losses, validation_accuracies\n",
        "\n",
        "    for epoch in range(start_epoch, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Inizializza hidden state\n",
        "            hidden = model.init_hidden(batch_size=inputs.size(0), device=device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, hidden = model(inputs, hidden)  # Passa hidden al modello\n",
        "            loss = criterion(outputs[:, -1, :], targets)  # Calcola la loss sull'ultima previsione\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Valutazione sul validation set\n",
        "        validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "\n",
        "        # Salva checkpoint\n",
        "        save_checkpoint(\n",
        "            model, optimizer, epoch, hyperparameters, \"Centralized/\",\n",
        "            data_to_save={\n",
        "                'validation_losses': validation_losses,\n",
        "                'validation_accuracies': validation_accuracies,\n",
        "                'train_losses': train_losses\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}, \")\n",
        "\n",
        "    # Valutazione sul test set\n",
        "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, validation_losses, validation_accuracies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Inizializza hidden state\n",
        "            hidden = model.init_hidden(batch_size=inputs.size(0), device=device)\n",
        "\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            loss = criterion(outputs[:, -1, :], targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs[:, -1, :].max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Centralized training"
      ],
      "metadata": {
        "id": "Rys0x0RL4xDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Always run before creating new datasets\n",
        "\n",
        "if os.path.exists(\"/content/leaf/\"):\n",
        "  # Use shutil.rmtree to remove the folder and its contents\n",
        "  shutil.rmtree(\"/content/leaf\")\n",
        "  print(f\"Successfully deleted folder leaf\")\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/maxfra01/leaf.git\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "preprocess_params = {\n",
        "        'sharding': 'iid',\n",
        "        'sf': 1.0,\n",
        "        't': 'sample',\n",
        "        'tf': 0.8,\n",
        "    } # Get the full-size dataset\n",
        "\n",
        "train_dataset_big = ShakespeareDataset(root=\"/content/leaf/data/shakespeare\", split=\"train\", preprocess_params=preprocess_params)\n",
        "test_dataset = ShakespeareDataset(root=\"/content/leaf/data/shakespeare\", split=\"test\", preprocess_params=preprocess_params)\n"
      ],
      "metadata": {
        "id": "2qfjvLkl-_P3",
        "outputId": "deb800db-ca04-4b7c-9fda-9cdcb8abd729",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'leaf'...\n",
            "remote: Enumerating objects: 772, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 772 (delta 0), reused 0 (delta 0), pack-reused 766 (from 1)\u001b[K\n",
            "Receiving objects: 100% (772/772), 6.78 MiB | 23.39 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n",
            "Running command: bash preprocess.sh -s iid --sf 1.0 -t sample --tf 0.8\n",
            "Absolute folder path: /content/leaf/data/shakespeare/data/train\n",
            "Unique labels: [' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '}']\n",
            "Running command: bash preprocess.sh -s iid --sf 1.0 -t sample --tf 0.8\n",
            "Absolute folder path: /content/leaf/data/shakespeare/data/test\n",
            "Unique labels: [' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '1', '2', '3', '4', '5', '6', '8', ':', ';', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.8\n",
        "WEIGHT_DECAY=1e-4\n",
        "EPOCHS = 2\n",
        "\n",
        "hyperparameters = f\"BS{BATCH_SIZE}_LR{LEARNING_RATE}_WD{WEIGHT_DECAY}_M{MOMENTUM}\"\n",
        "\n",
        "vocab_size = 100        # Il vocabolario deriva dalla dimensione dell'input (caratteri ASCII o token)\n",
        "embed_dim = 128         # Dimensione degli embedding\n",
        "hidden_size = 256       # Dimensione dello stato nascosto dell'LSTM\n",
        "num_layers = 2          # Numero di strati dell'LSTM\n",
        "\n",
        "model_shakespeare = ShakespeareRNN(vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
        "\n",
        "# Create the validation split\n",
        "indexes = range(0, len(train_dataset_big))\n",
        "splitting = train_test_split(indexes, train_size = 0.8, random_state = 42, shuffle = True)\n",
        "train_indexes = splitting[0]\n",
        "val_indexes = splitting[1]\n",
        "\n",
        "train_dataset = Subset(train_dataset_big, train_indexes)\n",
        "val_dataset = Subset(train_dataset_big, val_indexes)\n",
        "\n",
        "# Create Dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=2)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=2)\n",
        "\n",
        "# Set device and model parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_dim = len(train_dataset[0][0])\n",
        "output_dim = len(set(train_dataset_big.data['target']))  # Numero di classi\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_shakespeare.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses, val_accuracies = train_model(\n",
        "    model=model_shakespeare,\n",
        "    train_loader=train_dataloader,\n",
        "    validation_loader= val_dataloader,\n",
        "    test_loader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    criterion=criterion,\n",
        "    epochs=EPOCHS,\n",
        "    hyperparameters=hyperparameters\n",
        ")\n",
        "\n",
        "# Evaluation on test split\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(val_losses, label='Shakespeare Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Shakespare Val Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bUGr07HXneev",
        "outputId": "2f4593d9-8bfe-467c-efd1-f9a28223c991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found, Starting now...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1rnjkaAjcwQ"
      },
      "source": [
        "## Federate Learning classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJm8S7pjjcwR"
      },
      "outputs": [],
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "def plot_selected_clients_distribution(selected_clients_per_round, num_clients, hyperparameters):\n",
        "    \"\"\"Plotta la distribuzione dei client selezionati alla fine del processo.\"\"\"\n",
        "    counts = np.zeros(num_clients)\n",
        "\n",
        "    # Conta quante volte ogni client √® stato selezionato in tutti i round\n",
        "    for selected_clients in selected_clients_per_round:\n",
        "        for client in selected_clients:\n",
        "            counts[client] += 1\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(num_clients), counts, color='skyblue', edgecolor='black')\n",
        "    plt.title(\"Distribuzione dei Client Selezionati Durante il Federated Averaging\")\n",
        "    plt.xlabel(\"Client ID\")\n",
        "    plt.ylabel(\"Frequenza di Selezione\")\n",
        "    plt.grid(axis='y')\n",
        "    plt.savefig(f\"Shakespeare_Client_distribution_{hyperparameters}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, local_steps, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
        "\n",
        "    steps = 0\n",
        "    while steps < local_steps:\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "        if steps >= local_steps:\n",
        "          break\n",
        "    return self.model.state_dict()\n",
        "\n",
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data, val_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.val_data = val_data\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "    self.selected_clients_per_round = [] #clint selezionati per skewness\n",
        "\n",
        "  def federated_averaging(self, local_steps, batch_size, num_rounds, fraction_fit, skewness = None, hyperparameters = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "     # Carica il checkpoint se esiste\n",
        "    data_to_load = None\n",
        "    if skewness is  None:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "    else:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Skewed/\")\n",
        "\n",
        "    if data_to_load is not None:\n",
        "      self.round_losses = data_to_load['round_losses']\n",
        "      self.round_accuracies = data_to_load['round_accuracies']\n",
        "      self.selected_clients_per_round = data_to_load['selected_clients_per_round']\n",
        "\n",
        "\n",
        "    for round in range(start_epoch, num_rounds+1):\n",
        "\n",
        "      if skewness is not None:\n",
        "        probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False, p=probabilities)\n",
        "\n",
        "      else:\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False)\n",
        "\n",
        "      self.selected_clients_per_round.append([client.client_id for client in selected_clients])\n",
        "\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, local_steps, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model every 10 rounds\n",
        "      if round % 10 == 0:\n",
        "        loss, accuracy = evaluate_model(self.model, DataLoader(self.val_data, batch_size=batch_size, shuffle=True, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "        self.round_losses.append(loss)\n",
        "        self.round_accuracies.append(accuracy)\n",
        "        print(f\"Round {round}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        data_to_save = {\n",
        "          'round_losses': self.round_losses,\n",
        "          'round_accuracies': self.round_accuracies,\n",
        "          'selected_clients_per_round': [[client for client in round_clients] for round_clients in self.selected_clients_per_round]  # Serializziamo solo i client_id\n",
        "      }\n",
        "\n",
        "        if skewness is  None:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\", data_to_save)\n",
        "        else:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Skewed/\", data_to_save)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Evaluation on test set...\")\n",
        "    loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='Shakespeare Validation Loss')\n",
        "    plt.xlabel('Round (x10)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='Shakespeare Validation Accuracy')\n",
        "    plt.xlabel('Round (x10)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    if skewness is  None:\n",
        "      plt.savefig(f\"Shakespeare_fedavg_uniform{hyperparameters}.jpg\")\n",
        "    else:\n",
        "      plt.savefig(f\"Shakespeare_fedavg_skew{hyperparameters}.jpg\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    plot_selected_clients_distribution(self.selected_clients_per_round, len(self.clients), hyperparameters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ypHP_8fmyl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}