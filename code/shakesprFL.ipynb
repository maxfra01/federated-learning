{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbHNmOvzjcwC"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GTbCMkk8jcwK"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import drive\n",
        "\n",
        "# Set the working directory\n",
        "DIR_DATA = '/content/'\n",
        "os.chdir(DIR_DATA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "id": "2d0FdWHolo_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_DATA = \"./data\"\n",
        "CHECKPOINT_DIR = './checkpoints/'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -1}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath):\n",
        "        os.remove(previous_filepath)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca pi√π alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1  # Le epoche iniziano da 1"
      ],
      "metadata": {
        "id": "vVQ8Xif5lqil"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Dataset"
      ],
      "metadata": {
        "id": "j3i5KwH-pF1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, root, split, preprocess_params=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the dataset directory.\n",
        "            split (str): Dataset split, either 'train' or 'test'.\n",
        "            preprocess_params (dict, optional): Parameters for running preprocess.sh script. Keys include:\n",
        "                - sharding (str): 'iid' or 'niid' for data partitioning.\n",
        "                - iu (float): Fraction of users if i.i.d. sampling.\n",
        "                - sf (float): Fraction of data to sample.\n",
        "                - k (int): Minimum number of samples per user.\n",
        "                - t (str): 'user' or 'sample' for train-test partition.\n",
        "                - tf (float): Fraction of data in training set.\n",
        "                - raw (bool): Include raw text data.\n",
        "                - smplseed (int): Seed for sampling.\n",
        "                - spltseed (int): Seed for splitting.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.preprocess_params = preprocess_params or {}\n",
        "\n",
        "        # Ensure the working directory is set to the dataset folder\n",
        "        os.chdir(self.root)\n",
        "\n",
        "        # Run preprocessing script if needed\n",
        "        self._preprocess_data()\n",
        "\n",
        "        # Load the dataset\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"Runs preprocess.sh with the given parameters.\"\"\"\n",
        "        cmd = \"bash preprocess.sh\"\n",
        "\n",
        "        if 'sharding' in self.preprocess_params:\n",
        "            cmd += f\" -s {self.preprocess_params['sharding']}\"\n",
        "        if 'iu' in self.preprocess_params:\n",
        "            cmd += f\" --iu {self.preprocess_params['iu']}\"\n",
        "        if 'sf' in self.preprocess_params:\n",
        "            cmd += f\" --sf {self.preprocess_params['sf']}\"\n",
        "        if 'k' in self.preprocess_params:\n",
        "            cmd += f\" -k {self.preprocess_params['k']}\"\n",
        "        if 't' in self.preprocess_params:\n",
        "            cmd += f\" -t {self.preprocess_params['t']}\"\n",
        "        if 'tf' in self.preprocess_params:\n",
        "            cmd += f\" --tf {self.preprocess_params['tf']}\"\n",
        "        if 'raw' in self.preprocess_params and self.preprocess_params['raw']:\n",
        "            cmd += f\" --raw\"\n",
        "        if 'smplseed' in self.preprocess_params:\n",
        "            cmd += f\" --smplseed {self.preprocess_params['smplseed']}\"\n",
        "        if 'spltseed' in self.preprocess_params:\n",
        "            cmd += f\" --spltseed {self.preprocess_params['spltseed']}\"\n",
        "\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        os.system(cmd)\n",
        "        os.chdir(DIR_DATA)\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"Loads data from the JSON files into a pandas DataFrame.\"\"\"\n",
        "        file_path = os.path.join(self.root, \"data/all_data/all_data.json\")\n",
        "\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Convert JSON structure to a pandas DataFrame\n",
        "        records = []\n",
        "        for user, user_data in data['user_data'].items():\n",
        "            for x, y in zip(user_data['x'], user_data['y']):\n",
        "                records.append({\n",
        "                    'user': user,\n",
        "                    'input': x,\n",
        "                    'target': y\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        \"\"\"Returns the dataset as a pandas DataFrame.\"\"\"\n",
        "        return self.data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Retrieves a single sample by index.\"\"\"\n",
        "        return self.data.iloc[idx]"
      ],
      "metadata": {
        "id": "KNrpQePKpJOS"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('./leaf'):\n",
        "  !git clone https://github.com/maxfra01/leaf.git\n",
        "os.chdir(\"/content/\")\n",
        "preprocess_params = {\n",
        "        'sharding': 'niid',\n",
        "        'sf': 0.2,\n",
        "        'k': 0,\n",
        "        't': 'sample',\n",
        "        'tf': 0.8,\n",
        "        'raw': True\n",
        "    }\n",
        "\n",
        "dataset = ShakespeareDataset(root=\"leaf/data/shakespeare/\", split=\"train\", preprocess_params=preprocess_params)\n",
        "print(dataset.get_dataframe().head())\n"
      ],
      "metadata": {
        "id": "e1QXotugvu1o",
        "outputId": "07f04091-b7bd-42af-c671-b802f7d9888d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running command: bash preprocess.sh -s niid --sf 0.2 -k 0 -t sample --tf 0.8 --raw\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'leaf/data/shakespeare/data/all_data/all_data.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-6774843f3686>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShakespeareDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"leaf/data/shakespeare/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-836e052bd845>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, preprocess_params)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-836e052bd845>\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/all_data/all_data.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'leaf/data/shakespeare/data/all_data/all_data.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Model Architecture"
      ],
      "metadata": {
        "id": "5ASBPuONliim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers):\n",
        "        super(ShakespeareRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)  # Embed input\n",
        "        out, hidden = self.lstm(x, hidden)  # Pass through LSTM layers\n",
        "        out = self.fc(out)  # Fully connected layer for output\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "e6JWBdeWlmBT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPueHaDjcwN"
      },
      "source": [
        "## Centralized training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S8YKXaBSjcwP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Carica checkpoint se esiste\n",
        "    start_epoch = load_checkpoint(model, optimizer, hyperparameters,\"Centralized/\")\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Salva checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch, hyperparameters,\"Centralized/\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, test_losses, test_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the centralized model"
      ],
      "metadata": {
        "id": "bUGr07HXneev"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1rnjkaAjcwQ"
      },
      "source": [
        "## Federate Learning classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FJm8S7pjjcwR"
      },
      "outputs": [],
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, epochs, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "      #print(f\"Client {self.client_id}, Epoch {epoch+1}/{epochs}\")\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n",
        "\n",
        "class Server:\n",
        "  def __init__(self, model, clients, test_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "\n",
        "  def federated_averaging(self, epochs, batch_size, num_rounds, fraction_fit, skewness=None, hyperparameters = None, fedOptimizer=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "\n",
        "    # Carica il checkpoint se esiste\n",
        "    if skewness is  None:\n",
        "      start_epoch = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "    else:\n",
        "      start_epoch = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Skewed/\")\n",
        "\n",
        "\n",
        "    # Initialize variables for FedOptimizers\n",
        "    if fedOptimizer in {\"FedAdaGrad\", \"FedYogi\", \"FedAdam\"}:\n",
        "        optimizer_state = {\n",
        "            \"m\": {key: torch.zeros_like(value, dtype=torch.float32) for key, value in self.model.state_dict().items()},\n",
        "            \"v\": {key: torch.zeros_like(value, dtype=torch.float32) for key, value in self.model.state_dict().items()},\n",
        "        }\n",
        "        beta1 = 0.9  # Momentum parameter for Adam-based optimizers\n",
        "        beta2 = 0.999  # RMS parameter for Adam-based optimizers\n",
        "        lr = 0.01  # Learning rate\n",
        "        eps = 1e-8  # Small constant to prevent division by zero\n",
        "\n",
        "    for round in range(start_epoch, num_rounds):\n",
        "        print(f\"Round {round + 1}/{num_rounds}\")\n",
        "\n",
        "        if skewness is not None:\n",
        "            probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "            selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit * len(self.clients))),\n",
        "                                                replace=False, p=probabilities)\n",
        "        else:\n",
        "            selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit * len(self.clients))),\n",
        "                                                replace=False)\n",
        "\n",
        "        global_weights = self.model.state_dict()\n",
        "\n",
        "        # Simulate parallel client training\n",
        "        client_weights = {}\n",
        "        for client in selected_clients:\n",
        "            client_weights[client.client_id] = client.train(global_weights, epochs, batch_size)\n",
        "\n",
        "        # Aggregate client updates\n",
        "        total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "        aggregated_updates = {key: torch.zeros_like(value, dtype=torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "        for client in selected_clients:\n",
        "            scaling_factor = len(client.data) / total_data_size\n",
        "            for key in aggregated_updates.keys():\n",
        "                aggregated_updates[key] += scaling_factor * (client_weights[client.client_id][key] - global_weights[key])\n",
        "\n",
        "        # Apply selected FedOptimizer\n",
        "        if fedOptimizer == \"FedAdaGrad\":\n",
        "            for key in global_weights.keys():\n",
        "                optimizer_state[\"v\"][key] += aggregated_updates[key] ** 2\n",
        "                global_weights[key] += lr * aggregated_updates[key] / (torch.sqrt(optimizer_state[\"v\"][key]) + eps)\n",
        "\n",
        "        elif fedOptimizer == \"FedYogi\":\n",
        "            for key in global_weights.keys():\n",
        "                optimizer_state[\"v\"][key] -= (1 - beta2) * aggregated_updates[key] ** 2 * torch.sign(\n",
        "                    optimizer_state[\"v\"][key] - aggregated_updates[key] ** 2)\n",
        "                global_weights[key] += lr * aggregated_updates[key] / (torch.sqrt(optimizer_state[\"v\"][key]) + eps)\n",
        "\n",
        "        elif fedOptimizer == \"FedAdam\":\n",
        "            for key in global_weights.keys():\n",
        "                optimizer_state[\"m\"][key] = beta1 * optimizer_state[\"m\"][key] + (1 - beta1) * aggregated_updates[key]\n",
        "                optimizer_state[\"v\"][key] = beta2 * optimizer_state[\"v\"][key] + (1 - beta2) * aggregated_updates[key] ** 2\n",
        "                m_hat = optimizer_state[\"m\"][key] / (1 - beta1 ** (round + 1))\n",
        "                v_hat = optimizer_state[\"v\"][key] / (1 - beta2 ** (round + 1))\n",
        "                global_weights[key] += lr * m_hat / (torch.sqrt(v_hat) + eps)\n",
        "\n",
        "        else:  # Default to FedAvg\n",
        "            for key in global_weights.keys():\n",
        "                global_weights[key] += aggregated_updates[key]\n",
        "\n",
        "        # Update global model weights\n",
        "        self.model.load_state_dict(global_weights)\n",
        "\n",
        "        if skewness is  None:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\")\n",
        "        else:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Skewed/\")\n",
        "\n",
        "\n",
        "        # Evaluate global model\n",
        "        loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True),\n",
        "                                        nn.CrossEntropyLoss(), device)\n",
        "        self.round_losses.append(loss)\n",
        "        self.round_accuracies.append(accuracy)\n",
        "        print(f\"Round {round + 1}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='Test Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solver"
      ],
      "metadata": {
        "id": "QDWAowUBmxID"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ypHP_8fmyl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}