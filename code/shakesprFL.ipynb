{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbHNmOvzjcwC"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GTbCMkk8jcwK"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import drive\n",
        "\n",
        "# Set the working directory\n",
        "DIR_DATA = '/content/'\n",
        "os.chdir(DIR_DATA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "id": "2d0FdWHolo_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a Folder in the root directory\n",
        "!mkdir -p \"/content/drive/My Drive/My Folder/checkpoints_shakespeare\"\n",
        "\n",
        "DIR_DATA = \"./data\"\n",
        "CHECKPOINT_DIR = '/content/drive/My Drive/My Folder/checkpoints_shakespeare'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", data_to_save=None):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -1}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "    previous_filename_json = f\"model_epoch_{epoch -1}_params_{hyperparameters}.json\"\n",
        "    previous_filepath_json = os.path.join(subfolder_path, previous_filename_json)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath) and os.path.exists(previous_filepath_json):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "    with open(filepath_json, 'w') as json_file:\n",
        "      json.dump(data_to_save, json_file, indent=4)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1, None  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca più alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Trova e carica il file JSON associato\n",
        "        json_filename = latest_file.replace('.pth', '.json')\n",
        "        json_filepath = os.path.join(subfolder_path, json_filename)\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(f\"JSON data loaded: {json_filepath}\")\n",
        "        else:\n",
        "            print(f\"No JSON file found for: {latest_file}\")\n",
        "\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1, None  # Le epoche iniziano da 1\n",
        "\n"
      ],
      "metadata": {
        "id": "vVQ8Xif5lqil",
        "outputId": "96bb3b88-9af1-42b6-c320-e79391605f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-80fe6e4757d1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create a Folder in the root directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Dataset"
      ],
      "metadata": {
        "id": "j3i5KwH-pF1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_transform(text, max_length=100):\n",
        "    # Tokenizzazione semplice: converti ogni carattere in un valore numerico (es. ASCII)\n",
        "    tokenized = [ord(char) for char in text]\n",
        "\n",
        "    # Padding o Troncamento per lunghezza fissa\n",
        "    if len(tokenized) < max_length:\n",
        "        tokenized += [0] * (max_length - len(tokenized))  # Pad con zeri\n",
        "    else:\n",
        "        tokenized = tokenized[:max_length]  # Troncamento\n",
        "\n",
        "    # Converte in tensore\n",
        "    return torch.tensor(tokenized, dtype=torch.float)\n",
        "\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, root, split, preprocess_params=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the dataset directory.\n",
        "            split (str): Dataset split, either 'train' or 'test'.\n",
        "            preprocess_params (dict, optional): Parameters for running preprocess.sh script. Keys include:\n",
        "                - sharding (str): 'iid' or 'niid' for data partitioning.\n",
        "                - iu (float): Fraction of users if i.i.d. sampling.\n",
        "                - sf (float): Fraction of data to sample.\n",
        "                - k (int): Minimum number of samples per user.\n",
        "                - t (str): 'user' or 'sample' for train-test partition.\n",
        "                - tf (float): Fraction of data in training set.\n",
        "                - raw (bool): Include raw text data.\n",
        "                - smplseed (int): Seed for sampling.\n",
        "                - spltseed (int): Seed for splitting.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.preprocess_params = preprocess_params or {}\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = text_transform\n",
        "\n",
        "        # Ensure the working directory is set to the dataset folder\n",
        "        os.chdir(self.root)\n",
        "\n",
        "        # Run preprocessing script if needed\n",
        "        self._preprocess_data()\n",
        "\n",
        "        # Load the dataset\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"Runs preprocess.sh with the given parameters.\"\"\"\n",
        "        cmd = \"bash preprocess.sh\"\n",
        "\n",
        "        if 'sharding' in self.preprocess_params:\n",
        "            cmd += f\" -s {self.preprocess_params['sharding']}\"\n",
        "        if 'iu' in self.preprocess_params:\n",
        "            cmd += f\" --iu {self.preprocess_params['iu']}\"\n",
        "        if 'sf' in self.preprocess_params:\n",
        "            cmd += f\" --sf {self.preprocess_params['sf']}\"\n",
        "        if 'k' in self.preprocess_params:\n",
        "            cmd += f\" -k {self.preprocess_params['k']}\"\n",
        "        if 't' in self.preprocess_params:\n",
        "            cmd += f\" -t {self.preprocess_params['t']}\"\n",
        "        if 'tf' in self.preprocess_params:\n",
        "            cmd += f\" --tf {self.preprocess_params['tf']}\"\n",
        "        if 'raw' in self.preprocess_params and self.preprocess_params['raw']:\n",
        "            cmd += f\" --raw\"\n",
        "        if 'smplseed' in self.preprocess_params:\n",
        "            cmd += f\" --smplseed {self.preprocess_params['smplseed']}\"\n",
        "        if 'spltseed' in self.preprocess_params:\n",
        "            cmd += f\" --spltseed {self.preprocess_params['spltseed']}\"\n",
        "\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        os.system(cmd)\n",
        "        os.chdir(DIR_DATA)\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "      \"\"\"Loads data from the JSON file in the train or test folder, assuming only one file per folder.\"\"\"\n",
        "      # Identifica il file JSON nella directory specificata\n",
        "      folder_path = os.path.join(self.root,'data', self.split)\n",
        "      json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
        "\n",
        "      if len(json_files) != 1:\n",
        "          raise ValueError(f\"Expected exactly one JSON file in {folder_path}, but found {len(json_files)} files.\")\n",
        "\n",
        "      file_path = os.path.join(folder_path, json_files[0])\n",
        "\n",
        "      # Carica i dati dal file JSON\n",
        "      with open(file_path, 'r') as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      # Converti la struttura JSON in un DataFrame di pandas\n",
        "      records = []\n",
        "      for user, user_data in data['user_data'].items():\n",
        "          for x, y in zip(user_data['x'], user_data['y']):\n",
        "              records.append({\n",
        "                  'user': user,\n",
        "                  'input': x,\n",
        "                  'target': y\n",
        "              })\n",
        "\n",
        "      return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        \"\"\"Returns the dataset as a pandas DataFrame.\"\"\"\n",
        "        return self.data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'input': self.data.iloc[idx]['input'],\n",
        "            'target': self.data.iloc[idx]['target']\n",
        "        }\n",
        "\n",
        "        # Applica la trasformazione agli input (e.g., tokenizzazione e padding)\n",
        "        if self.transform:\n",
        "            sample['input'] = self.transform(sample['input'])\n",
        "\n",
        "        # Converte i target in tensori\n",
        "        sample['target'] = torch.tensor(sample['target'], dtype=torch.long)\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "KNrpQePKpJOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Model Architecture"
      ],
      "metadata": {
        "id": "5ASBPuONliim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers):\n",
        "        super(ShakespeareRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)  # Embed input\n",
        "        out, hidden = self.lstm(x, hidden)  # Pass through LSTM layers\n",
        "        out = self.fc(out)  # Fully connected layer for output\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "e6JWBdeWlmBT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPueHaDjcwN"
      },
      "source": [
        "## Centralized training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S8YKXaBSjcwP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Carica checkpoint se esiste\n",
        "    start_epoch = load_checkpoint(model, optimizer, hyperparameters,\"Centralized/\")\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Salva checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch, hyperparameters,\"Centralized/\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, test_losses, test_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Centralized training"
      ],
      "metadata": {
        "id": "Rys0x0RL4xDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Always run before creating new datasets\n",
        "\n",
        "if os.path.exists(\"/content/leaf/\"):\n",
        "  # Use shutil.rmtree to remove the folder and its contents\n",
        "  shutil.rmtree(\"/content/leaf\")\n",
        "  print(f\"Successfully deleted folder leaf\")\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/maxfra01/leaf.git\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "preprocess_params = {\n",
        "        'sharding': 'iid',\n",
        "        'sf': 1.0,\n",
        "        't': 'sample',\n",
        "        'tf': 0.8,\n",
        "    } # Get the full-size dataset\n",
        "\n",
        "train_dataset_big = ShakespeareDataset(root=\"leaf/data/shakespeare/\", split=\"train\", preprocess_params=preprocess_params)\n",
        "test_dataset = ShakespeareDataset(root=\"leaf/data/shakespeare\", split=\"test\", preprocess_params=preprocess_params)\n"
      ],
      "metadata": {
        "id": "2qfjvLkl-_P3",
        "outputId": "588f6b60-e86f-4b0f-d55a-ed6433875dc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully deleted folder leaf\n",
            "Cloning into 'leaf'...\n",
            "remote: Enumerating objects: 772, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 772 (delta 0), reused 0 (delta 0), pack-reused 766 (from 1)\u001b[K\n",
            "Receiving objects: 100% (772/772), 6.78 MiB | 13.87 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n",
            "Running command: bash preprocess.sh -s iid --sf 1.0 -t sample --tf 0.8\n",
            "Running command: bash preprocess.sh -s iid --sf 1.0 -t sample --tf 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.8\n",
        "WEIGHT_DECAY=1e-4\n",
        "EPOCHS = 50\n",
        "\n",
        "hyperparameters = f\"BS{BATCH_SIZE}_LR{LEARNING_RATE}_WD{WEIGHT_DECAY}_M{MOMENTUM}\"\n",
        "\n",
        "\n",
        "# Create the validation split\n",
        "indexes = range(0, len(train_dataset_big))\n",
        "splitting = train_test_split(indexes, train_size = 0.8, random_state = 42, shuffle = True)\n",
        "train_indexes = splitting[0]\n",
        "val_indexes = splitting[1]\n",
        "\n",
        "train_dataset = Subset(train_dataset_big, train_indexes)\n",
        "val_dataset = Subset(train_dataset_big, val_indexes)\n",
        "\n",
        "# Create Dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Set device and model parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_dim = len(train_dataset[0]['input'])\n",
        "output_dim = len(set(train_dataset_big.data['target']))  # Numero di classi\n",
        "\n",
        "model_shakespeare = ShakespeareRNN(input_dim=input_dim, output_dim=output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_shakespeare.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "# Train the model\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "train_losses, val_losses, val_accuracies = train_model(\n",
        "    model=model_shakespeare,\n",
        "    train_loader=train_dataloader,\n",
        "    test_loader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    criterion=criterion,\n",
        "    epochs=EPOCHS,\n",
        "    hyperparameters=hyperparameters\n",
        ")\n",
        "\n",
        "# Evaluation on test split\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
        "\n",
        "test_loss, test_accuracy = evaluate_model(model_shakespeare, test_dataloader, criterion, device)\n",
        "\n",
        "print(\"\\nFinal Model Evaluation on Test Set:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(val_losses, label='Shakespeare Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Shakespare Val Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bUGr07HXneev",
        "outputId": "8eef7658-a3a2-4cc7-a0c6-309acd70eb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "new(): invalid data type 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d99f68ed975c>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Set device and model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_big\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Numero di classi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_T_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-203d8d4df374>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Converte i target in tensori\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1rnjkaAjcwQ"
      },
      "source": [
        "## Federate Learning classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FJm8S7pjjcwR"
      },
      "outputs": [],
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "def plot_selected_clients_distribution(selected_clients_per_round, num_clients, hyperparameters):\n",
        "    \"\"\"Plotta la distribuzione dei client selezionati alla fine del processo.\"\"\"\n",
        "    counts = np.zeros(num_clients)\n",
        "\n",
        "    # Conta quante volte ogni client è stato selezionato in tutti i round\n",
        "    for selected_clients in selected_clients_per_round:\n",
        "        for client in selected_clients:\n",
        "            counts[client] += 1\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(num_clients), counts, color='skyblue', edgecolor='black')\n",
        "    plt.title(\"Distribuzione dei Client Selezionati Durante il Federated Averaging\")\n",
        "    plt.xlabel(\"Client ID\")\n",
        "    plt.ylabel(\"Frequenza di Selezione\")\n",
        "    plt.grid(axis='y')\n",
        "    plt.savefig(f\"CIFAR100_Client_distribution_{hyperparameters}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, local_steps, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
        "    steps = 0  # Track the number of steps\n",
        "    while steps < local_steps:\n",
        "      for inputs, targets in trainloader:\n",
        "          if steps >= local_steps:  # Stop after completing the required steps\n",
        "              break\n",
        "          inputs, targets = inputs.to(device), targets.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          outputs = self.model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          steps += 1\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n",
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "    self.selected_clients_per_round = [] #clint selezionati per skewness\n",
        "\n",
        "  def federated_averaging(self, local_steps, batch_size, num_rounds, fraction_fit, skewness = None, hyperparameters = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "     # Carica il checkpoint se esiste\n",
        "    data_to_load = None\n",
        "    if skewness is  None:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "    else:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Skewed/\")\n",
        "\n",
        "    if data_to_load is not None:\n",
        "      self.round_losses = data_to_load['round_losses']\n",
        "      self.round_accuracies = data_to_load['round_accuracies']\n",
        "      self.selected_clients_per_round = data_to_load['selected_clients_per_round']\n",
        "\n",
        "\n",
        "    for round in range(start_epoch, num_rounds+1):\n",
        "\n",
        "      if skewness is not None:\n",
        "        probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False, p=probabilities)\n",
        "\n",
        "      else:\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False)\n",
        "\n",
        "      self.selected_clients_per_round.append([client.client_id for client in selected_clients])\n",
        "\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, local_steps, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model each 10 rounds\n",
        "      if round % 10 == 0:\n",
        "        loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "        self.round_losses.append(loss)\n",
        "        self.round_accuracies.append(accuracy)\n",
        "        print(f\"Round {round}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        data_to_save = {\n",
        "          'round_losses': self.round_losses,\n",
        "          'round_accuracies': self.round_accuracies,\n",
        "          'selected_clients_per_round': [[client for client in round_clients] for round_clients in self.selected_clients_per_round]  # Serializziamo solo i client_id\n",
        "      }\n",
        "\n",
        "        if skewness is  None:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\", data_to_save)\n",
        "        else:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Skewed/\", data_to_save)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='CIFAR-100 Test Loss')\n",
        "    plt.xlabel('Round (x10)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='CIFAR-100 Test Accuracy')\n",
        "    plt.xlabel('Round (x10)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    if skewness is  None:\n",
        "      plt.savefig(f\"CIFAR100_fedavg_uniform{hyperparameters}.jpg\")\n",
        "    else:\n",
        "      plt.savefig(f\"CIFAR100_fedavg_skew{hyperparameters}.jpg\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    plot_selected_clients_distribution(self.selected_clients_per_round, len(self.clients), hyperparameters)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ypHP_8fmyl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}