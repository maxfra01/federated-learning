{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbHNmOvzjcwC"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GTbCMkk8jcwK",
        "outputId": "65c751c1-d433-4b18-dad9-70078991d75a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries for data processing, ML, and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "import kagglehub\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "import string\n",
        "import itertools\n",
        "\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "\n",
        "# Suppressing warnings for a cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Download Shakespeare Dataset\n",
        "path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
        "DATA_PATH = os.path.join(path, \"t8.shakespeare.txt\")\n",
        "DATA_DIR = \"data/\"\n",
        "\n",
        "TRAIN_FRACTION = 0.9\n",
        "SEQ_LEN = 80\n",
        "N_VOCAB = 90"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "id": "2d0FdWHolo_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a Folder in the root directory\n",
        "!mkdir -p \"/content/drive/My Drive/My Folder/checkpoints_shakespeare\"\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/My Drive/My Folder/checkpoints_shakespeare'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", data_to_save=None):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -1}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "    previous_filename_json = f\"model_epoch_{epoch -1}_params_{hyperparameters}.json\"\n",
        "    previous_filepath_json = os.path.join(subfolder_path, previous_filename_json)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath) and os.path.exists(previous_filepath_json):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "    with open(filepath_json, 'w') as json_file:\n",
        "      json.dump(data_to_save, json_file, indent=4)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1, None  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca più alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Trova e carica il file JSON associato\n",
        "        json_filename = latest_file.replace('.pth', '.json')\n",
        "        json_filepath = os.path.join(subfolder_path, json_filename)\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(f\"JSON data loaded: {json_filepath}\")\n",
        "        else:\n",
        "            print(f\"No JSON file found for: {latest_file}\")\n",
        "\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1, None  # Le epoche iniziano da 1\n",
        "\n"
      ],
      "metadata": {
        "id": "vVQ8Xif5lqil",
        "outputId": "b91e6cd0-3a2b-4320-a994-faaa02105383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Processing"
      ],
      "metadata": {
        "id": "cd7atWk1FPlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_FRACTION = 0.9\n",
        "\n",
        "CHARACTER_RE = re.compile(r'^  ([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Matches character lines\n",
        "CONT_RE = re.compile(r'^    (.*)')  # Matches continuation lines\n",
        "COE_CHARACTER_RE = re.compile(r'^([a-zA-Z][a-zA-Z ]*)\\. (.*)')  # Special regex for Comedy of Errors\n",
        "COE_CONT_RE = re.compile(r'^(.*)')  # Continuation for Comedy of Errors\n",
        "\n",
        "def parse_shakespeare_file(filepath):\n",
        "    \"\"\"\n",
        "    Reads and splits Shakespeare's text into plays, characters, and their dialogues.\n",
        "    Returns training and test datasets based on the specified fraction.\n",
        "    \"\"\"\n",
        "    with open(filepath, \"r\") as f:\n",
        "        content = f.read()\n",
        "    plays, _ = _split_into_plays(content)  # Split the text into plays\n",
        "    _, train_examples, test_examples = _get_train_test_by_character(\n",
        "        plays, test_fraction=1 - TRAIN_FRACTION\n",
        "    )\n",
        "    return train_examples, test_examples\n",
        "\n",
        "def _split_into_plays(shakespeare_full):\n",
        "    \"\"\"\n",
        "    Splits the full Shakespeare text into individual plays and characters' dialogues.\n",
        "    Handles special parsing for \"The Comedy of Errors\".\n",
        "    \"\"\"\n",
        "    plays = []\n",
        "    slines = shakespeare_full.splitlines(True)[1:]  # Skip the first line (title/header)\n",
        "    current_character = None\n",
        "    comedy_of_errors = False\n",
        "\n",
        "    for i, line in enumerate(slines):\n",
        "        # Detect play titles and initialize character dictionary\n",
        "        if \"by William Shakespeare\" in line:\n",
        "            current_character = None\n",
        "            characters = defaultdict(list)\n",
        "            title = slines[i - 2].strip() if slines[i - 2].strip() else slines[i - 3].strip()\n",
        "            comedy_of_errors = title == \"THE COMEDY OF ERRORS\"\n",
        "            plays.append((title, characters))\n",
        "            continue\n",
        "\n",
        "        # Match character lines or continuation lines\n",
        "        match = _match_character_regex(line, comedy_of_errors)\n",
        "        if match:\n",
        "            character, snippet = match.group(1).upper(), match.group(2)\n",
        "            if not (comedy_of_errors and character.startswith(\"ACT \")):\n",
        "                characters[character].append(snippet)\n",
        "                current_character = character\n",
        "        elif current_character:\n",
        "            match = _match_continuation_regex(line, comedy_of_errors)\n",
        "            if match:\n",
        "                characters[current_character].append(match.group(1))\n",
        "\n",
        "    # Filter out plays with insufficient dialogue data\n",
        "    return [play for play in plays if len(play[1]) > 1], []\n",
        "\n",
        "def _match_character_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches character dialogues, with special handling for 'The Comedy of Errors'.\"\"\"\n",
        "    return COE_CHARACTER_RE.match(line) if comedy_of_errors else CHARACTER_RE.match(line)\n",
        "\n",
        "def _match_continuation_regex(line, comedy_of_errors=False):\n",
        "    \"\"\"Matches continuation lines of dialogues.\"\"\"\n",
        "    return COE_CONT_RE.match(line) if comedy_of_errors else CONT_RE.match(line)\n",
        "\n",
        "def _get_train_test_by_character(plays, test_fraction=0.2):\n",
        "    \"\"\"\n",
        "    Splits dialogues by characters into training and testing datasets.\n",
        "    Ensures each character has at least one example in the training set.\n",
        "    \"\"\"\n",
        "    all_train_examples = defaultdict(list)\n",
        "    all_test_examples = defaultdict(list)\n",
        "\n",
        "    def add_examples(example_dict, example_tuple_list):\n",
        "        \"\"\"Adds examples to the respective dataset dictionary.\"\"\"\n",
        "        for play, character, sound_bite in example_tuple_list:\n",
        "            example_dict[f\"{play}_{character}\".replace(\" \", \"_\")].append(sound_bite)\n",
        "\n",
        "    for play, characters in plays:\n",
        "        for character, sound_bites in characters.items():\n",
        "            examples = [(play, character, sound_bite) for sound_bite in sound_bites]\n",
        "            if len(examples) <= 2:\n",
        "                continue\n",
        "\n",
        "            # Calculate the number of test samples\n",
        "            num_test = max(1, int(len(examples) * test_fraction))\n",
        "            num_test = min(num_test, len(examples) - 1)  # Ensure at least one training example\n",
        "\n",
        "            # Split into train and test sets\n",
        "            train_examples = examples[:-num_test]\n",
        "            test_examples = examples[-num_test:]\n",
        "\n",
        "            add_examples(all_train_examples, train_examples)\n",
        "            add_examples(all_test_examples, test_examples)\n",
        "\n",
        "    return {}, all_train_examples, all_test_examples\n",
        "\n",
        "\n",
        "def letter_to_vec(c, n_vocab=128):\n",
        "    \"\"\"Converts a single character to a vector index based on the vocabulary size.\"\"\"\n",
        "    return ord(c) % n_vocab\n",
        "\n",
        "def word_to_indices(word, n_vocab=128):\n",
        "    \"\"\"\n",
        "    Converts a word or list of words into a list of indices.\n",
        "    Each character is mapped to an index based on the vocabulary size.\n",
        "    \"\"\"\n",
        "    if isinstance(word, list):  # If input is a list of words\n",
        "        res = []\n",
        "        for stringa in word:\n",
        "            res.extend([ord(c) % n_vocab for c in stringa])  # Convert each word to indices\n",
        "        return res\n",
        "    else:  # If input is a single word\n",
        "        return [ord(c) % n_vocab for c in word]\n",
        "\n",
        "def process_x(raw_x_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw input data into padded sequences of indices.\n",
        "    Ensures all sequences are of uniform length.\n",
        "    \"\"\"\n",
        "    x_batch = [word_to_indices(word, n_vocab) for word in raw_x_batch]\n",
        "    x_batch = [x[:seq_len] + [0] * (seq_len - len(x)) for x in x_batch]\n",
        "    return torch.tensor(x_batch, dtype=torch.long)\n",
        "\n",
        "\n",
        "def process_y(raw_y_batch, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Processes raw target data into padded sequences of indices.\n",
        "    Shifts the sequence by one character to the right.\n",
        "    y[1:seq_len + 1] takes the input data, right shift of an\n",
        "    element and uses the next element of the sequence to fill\n",
        "    and at the end (with [0]) final padding (zeros) are (eventually)\n",
        "    added to reach the desired sequence length.\n",
        "    \"\"\"\n",
        "    y_batch = [word_to_indices(word, n_vocab) for word in raw_y_batch]\n",
        "    y_batch = [y[1:seq_len + 1] + [0] * (seq_len - len(y[1:seq_len + 1])) for y in y_batch]  # Shifting and final padding\n",
        "    return torch.tensor(y_batch, dtype=torch.long)\n",
        "\n",
        "def create_batches(data, batch_size, seq_len, n_vocab):\n",
        "    \"\"\"\n",
        "    Creates batches of input and target data from dialogues.\n",
        "    Each batch contains sequences of uniform length.\n",
        "    \"\"\"\n",
        "    x_batches = []\n",
        "    y_batches = []\n",
        "    dialogues = list(data.values())\n",
        "    random.shuffle(dialogues)  # Shuffle to ensure randomness in batches\n",
        "\n",
        "    batch = []\n",
        "    for dialogue in dialogues:\n",
        "        batch.append(dialogue)\n",
        "        if len(batch) == batch_size:\n",
        "            x_batch = process_x(batch, seq_len, n_vocab)\n",
        "            y_batch = process_y(batch, seq_len, n_vocab)\n",
        "            x_batches.append(x_batch)\n",
        "            y_batches.append(y_batch)\n",
        "            batch = []\n",
        "\n",
        "    # Add the last batch if it's not full\n",
        "    if batch:\n",
        "        x_batch = process_x(batch, seq_len, n_vocab)\n",
        "        y_batch = process_y(batch, seq_len, n_vocab)\n",
        "        x_batches.append(x_batch)\n",
        "        y_batches.append(y_batch)\n",
        "\n",
        "    return x_batches, y_batches"
      ],
      "metadata": {
        "id": "UIZtiSdYFOVu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Dataset"
      ],
      "metadata": {
        "id": "j3i5KwH-pF1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, data, seq_len, n_vocab):\n",
        "        self.data = list(data.values())  # Convert the dictionary values to a list\n",
        "        self.seq_len = seq_len  # Sequence length for the model\n",
        "        self.n_vocab = n_vocab  # Vocabulary size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dialogue = self.data[idx]\n",
        "        x = process_x([dialogue], self.seq_len, self.n_vocab)[0]\n",
        "        y = process_y([dialogue], self.seq_len, self.n_vocab)[0]\n",
        "        return x, y\n",
        "\n",
        "data_train, data_test = parse_shakespeare_file(DATA_PATH)\n",
        "dataset = ShakespeareDataset(data_train, seq_len=80, n_vocab=90)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "KNrpQePKpJOS",
        "outputId": "b33407d5-0469-4f26-aa58-eb2394fced20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1164"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Model Architecture"
      ],
      "metadata": {
        "id": "5ASBPuONliim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size=90, embed_dim=8, lstm_hidden_dim=256, seq_len=80, batch_size=32):\n",
        "        super(ShakespeareLSTM, self).__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
        "\n",
        "        # First LSTM layer\n",
        "        self.lstm1 = nn.LSTM(input_size=embed_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Second LSTM layer\n",
        "        self.lstm2 = nn.LSTM(input_size=lstm_hidden_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Dense output layer\n",
        "        self.dense = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"\n",
        "        Inizializza lo stato nascosto e la cella della LSTM come tensori di zeri.\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(2, batch_size, self.lstm_hidden_dim).to(self.device)\n",
        "        c0 = torch.zeros(2, batch_size, self.lstm_hidden_dim).to(self.device)\n",
        "        return (h0, c0)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # x: (batch_size, seq_len)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        x, hidden = self.lstm1(x, hidden)  # (batch_size, seq_len, lstm_hidden_dim)\n",
        "\n",
        "        x, hidden = self.lstm2(x, hidden)  # (batch_size, seq_len, lstm_hidden_dim)\n",
        "\n",
        "        x = self.dense(x)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return x, hidden"
      ],
      "metadata": {
        "id": "e6JWBdeWlmBT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPueHaDjcwN"
      },
      "source": [
        "## Centralized training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "S8YKXaBSjcwP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, validation_losses, validation_accuracies = [], [], []\n",
        "\n",
        "    # Carica checkpoint se esiste\n",
        "    start_epoch, json_data = load_checkpoint(model, optimizer, hyperparameters, \"Centralized/\")\n",
        "    if json_data is not None:\n",
        "        validation_losses = json_data.get('validation_losses', [])\n",
        "        validation_accuracies = json_data.get('validation_accuracies', [])\n",
        "        train_losses = json_data.get('train_losses', [])\n",
        "\n",
        "    if start_epoch >= epochs:\n",
        "        print(f\"Checkpoint trovato, configurazione già completata. Valutazione solo sul validation set.\")\n",
        "        validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "        return train_losses, validation_losses, validation_accuracies\n",
        "\n",
        "    import time\n",
        "\n",
        "    for epoch in range(start_epoch, epochs + 1):\n",
        "\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        x_batches, y_batches = create_batches(train_data, BATCH_SIZE, SEQ_LEN, N_VOCAB)\n",
        "\n",
        "        for x_batch, y_batch in zip(x_batches, y_batches):\n",
        "\n",
        "            inputs, targets = x_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, _ = model(inputs)\n",
        "            outputs = outputs.view(-1, 90)\n",
        "            targets = targets.view(-1)\n",
        "            loss = criterion(outputs, targets)  # Calcola la loss sull'ultima previsione\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Valutazione sul validation set\n",
        "        validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "\n",
        "        # Salva checkpoint\n",
        "        save_checkpoint(\n",
        "            model, optimizer, epoch, hyperparameters, \"Centralized/\",\n",
        "            data_to_save={\n",
        "                'validation_losses': validation_losses,\n",
        "                'validation_accuracies': validation_accuracies,\n",
        "                'train_losses': train_losses\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}, \")\n",
        "\n",
        "    # Valutazione sul test set\n",
        "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, validation_losses, validation_accuracies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs, _ = model(inputs)\n",
        "            outputs = outputs.view(-1, model.vocab_size)\n",
        "            targets = targets.view(-1)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Centralized training"
      ],
      "metadata": {
        "id": "Rys0x0RL4xDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Always run before creating new datasets\n",
        "\n",
        "if os.path.exists(\"/content/leaf/\"):\n",
        "  # Use shutil.rmtree to remove the folder and its contents\n",
        "  shutil.rmtree(\"/content/leaf\")\n",
        "  print(f\"Successfully deleted folder leaf\")\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/maxfra01/leaf.git\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "preprocess_params = {\n",
        "        'sharding': 'iid',\n",
        "        'sf': 1.0,\n",
        "        'iu': 0.1,\n",
        "        't': 'sample',\n",
        "        'tf': 0.8,\n",
        "    } # Get the full-size dataset\n",
        "\n",
        "train_dataset_big = CentralizedShakespeareDataset(root=\"/content/leaf/data/shakespeare\", split=\"train\", preprocess_params=preprocess_params)\n",
        "test_dataset = CentralizedShakespeareDataset(root=\"/content/leaf/data/shakespeare\", split=\"test\", preprocess_params=preprocess_params)\n"
      ],
      "metadata": {
        "id": "2qfjvLkl-_P3",
        "outputId": "459a2a77-55ed-437c-bf51-3ebc1b386281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully deleted folder leaf\n",
            "Cloning into 'leaf'...\n",
            "remote: Enumerating objects: 772, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 772 (delta 0), reused 0 (delta 0), pack-reused 766 (from 1)\u001b[K\n",
            "Receiving objects: 100% (772/772), 6.78 MiB | 28.94 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n",
            "Running command: bash preprocess.sh -s iid --iu 0.1 --sf 1.0 -t sample --tf 0.8\n",
            "Absolute folder path: /content/leaf/data/shakespeare/data/train\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text_to_indexes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-b7394b9652f6>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     } # Get the full-size dataset\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain_dataset_big\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCentralizedShakespeareDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/leaf/data/shakespeare\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCentralizedShakespeareDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/leaf/data/shakespeare\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-111-d6f6cb1d30b5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, seq_len, preprocess_params)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Carica i dati centralizzati\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_centralized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mall_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-111-d6f6cb1d30b5>\u001b[0m in \u001b[0;36m_load_centralized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muser_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mcombined_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_to_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Input e target come tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_to_indexes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.1\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY=1e-4\n",
        "EPOCHS = 20\n",
        "\n",
        "SEQ_LEN = 80\n",
        "N_VOCAB = 90\n",
        "\n",
        "hyperparameters = f\"BS{BATCH_SIZE}_LR{LEARNING_RATE}_WD{WEIGHT_DECAY}_M{MOMENTUM}\"\n",
        "\n",
        "\n",
        "model_shakespeare = ShakespeareLSTM()\n",
        "\n",
        "train_data, test_data = parse_shakespeare_file(DATA_PATH)\n",
        "\n",
        "train_dataset = ShakespeareDataset(train_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "test_dataset = ShakespeareDataset(test_data, seq_len=SEQ_LEN, n_vocab=N_VOCAB)\n",
        "\n",
        "# Split the train dataset into train and validation:\n",
        "train_size = int(TRAIN_FRACTION * len(train_dataset))  # 90%\n",
        "valid_size = len(train_dataset) - train_size  # 10%\n",
        "#random split:\n",
        "train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "# Creation of the DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model_shakespeare.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    momentum=MOMENTUM,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses, val_accuracies = train_model(\n",
        "    model=model_shakespeare,\n",
        "    train_loader=train_dataloader,\n",
        "    validation_loader= val_dataloader,\n",
        "    test_loader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    criterion=criterion,\n",
        "    epochs=EPOCHS,\n",
        "    hyperparameters=hyperparameters\n",
        ")\n",
        "\n",
        "# Evaluation on test split\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(val_losses, label='Shakespeare Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Shakespare Val Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bUGr07HXneev",
        "outputId": "9b1231cb-5ca5-459b-e6cd-123a8d3dbf36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found, Starting now...\n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_1_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 1/20, Train Loss: 934.4789, Validation Loss: 3.0023, Validation Accuracy: 0.2064, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_2_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 2/20, Train Loss: 814.6838, Validation Loss: 2.5952, Validation Accuracy: 0.2946, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_3_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 3/20, Train Loss: 731.7919, Validation Loss: 2.4180, Validation Accuracy: 0.3390, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_4_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 4/20, Train Loss: 688.5215, Validation Loss: 2.2971, Validation Accuracy: 0.3757, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_5_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 5/20, Train Loss: 653.3202, Validation Loss: 2.1837, Validation Accuracy: 0.3937, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_6_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 6/20, Train Loss: 624.7701, Validation Loss: 2.1067, Validation Accuracy: 0.4178, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_7_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 7/20, Train Loss: 599.2249, Validation Loss: 2.0178, Validation Accuracy: 0.4399, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_8_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 8/20, Train Loss: 578.3688, Validation Loss: 1.9555, Validation Accuracy: 0.4484, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_9_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 9/20, Train Loss: 561.2926, Validation Loss: 1.8857, Validation Accuracy: 0.4716, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_10_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 10/20, Train Loss: 545.7514, Validation Loss: 1.8291, Validation Accuracy: 0.4838, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_11_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 11/20, Train Loss: 532.6265, Validation Loss: 1.7862, Validation Accuracy: 0.4931, \n",
            "Checkpoint salvato: /content/drive/My Drive/My Folder/checkpoints_shakespeare/Centralized/model_epoch_12_params_BS4_LR0.1_WD0.0001_M0.9.pth\n",
            "Epoch 12/20, Train Loss: 520.5994, Validation Loss: 1.7516, Validation Accuracy: 0.5016, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1rnjkaAjcwQ"
      },
      "source": [
        "## Federate Learning classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJm8S7pjjcwR"
      },
      "outputs": [],
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "def plot_selected_clients_distribution(selected_clients_per_round, num_clients, hyperparameters):\n",
        "    \"\"\"Plotta la distribuzione dei client selezionati alla fine del processo.\"\"\"\n",
        "    counts = np.zeros(num_clients)\n",
        "\n",
        "    # Conta quante volte ogni client è stato selezionato in tutti i round\n",
        "    for selected_clients in selected_clients_per_round:\n",
        "        for client in selected_clients:\n",
        "            counts[client] += 1\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(num_clients), counts, color='skyblue', edgecolor='black')\n",
        "    plt.title(\"Distribuzione dei Client Selezionati Durante il Federated Averaging\")\n",
        "    plt.xlabel(\"Client ID\")\n",
        "    plt.ylabel(\"Frequenza di Selezione\")\n",
        "    plt.grid(axis='y')\n",
        "    plt.savefig(f\"Shakespeare_Client_distribution_{hyperparameters}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, local_steps, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
        "\n",
        "    steps = 0\n",
        "    while steps < local_steps:\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "        if steps >= local_steps:\n",
        "          break\n",
        "    return self.model.state_dict()\n",
        "\n",
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data, val_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.val_data = val_data\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "    self.selected_clients_per_round = [] #clint selezionati per skewness\n",
        "\n",
        "  def federated_averaging(self, local_steps, batch_size, num_rounds, fraction_fit, skewness = None, hyperparameters = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "     # Carica il checkpoint se esiste\n",
        "    data_to_load = None\n",
        "    if skewness is  None:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "    else:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Skewed/\")\n",
        "\n",
        "    if data_to_load is not None:\n",
        "      self.round_losses = data_to_load['round_losses']\n",
        "      self.round_accuracies = data_to_load['round_accuracies']\n",
        "      self.selected_clients_per_round = data_to_load['selected_clients_per_round']\n",
        "\n",
        "\n",
        "    for round in range(start_epoch, num_rounds+1):\n",
        "\n",
        "      if skewness is not None:\n",
        "        probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False, p=probabilities)\n",
        "\n",
        "      else:\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False)\n",
        "\n",
        "      self.selected_clients_per_round.append([client.client_id for client in selected_clients])\n",
        "\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, local_steps, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model every 10 rounds\n",
        "      if round % 10 == 0:\n",
        "        loss, accuracy = evaluate_model(self.model, DataLoader(self.val_data, batch_size=batch_size, shuffle=True, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "        self.round_losses.append(loss)\n",
        "        self.round_accuracies.append(accuracy)\n",
        "        print(f\"Round {round}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        data_to_save = {\n",
        "          'round_losses': self.round_losses,\n",
        "          'round_accuracies': self.round_accuracies,\n",
        "          'selected_clients_per_round': [[client for client in round_clients] for round_clients in self.selected_clients_per_round]  # Serializziamo solo i client_id\n",
        "      }\n",
        "\n",
        "        if skewness is  None:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\", data_to_save)\n",
        "        else:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Skewed/\", data_to_save)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Evaluation on test set...\")\n",
        "    loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "    print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='Shakespeare Validation Loss')\n",
        "    plt.xlabel('Round (x10)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='Shakespeare Validation Accuracy')\n",
        "    plt.xlabel('Round (x10)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    if skewness is  None:\n",
        "      plt.savefig(f\"Shakespeare_fedavg_uniform{hyperparameters}.jpg\")\n",
        "    else:\n",
        "      plt.savefig(f\"Shakespeare_fedavg_skew{hyperparameters}.jpg\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    plot_selected_clients_distribution(self.selected_clients_per_round, len(self.clients), hyperparameters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ypHP_8fmyl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}