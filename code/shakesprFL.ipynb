{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbHNmOvzjcwC"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GTbCMkk8jcwK"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import drive\n",
        "\n",
        "# Set the working directory\n",
        "DIR_DATA = '/content/'\n",
        "os.chdir(DIR_DATA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "id": "2d0FdWHolo_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_DATA = \"./data\"\n",
        "CHECKPOINT_DIR = './checkpoints/'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -1}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath):\n",
        "        os.remove(previous_filepath)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca pi√π alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1  # Le epoche iniziano da 1"
      ],
      "metadata": {
        "id": "vVQ8Xif5lqil"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Dataset"
      ],
      "metadata": {
        "id": "j3i5KwH-pF1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_transform(text, max_length=100):\n",
        "    # Tokenizzazione semplice: converti ogni carattere in un valore numerico (es. ASCII)\n",
        "    tokenized = [ord(char) for char in text]\n",
        "\n",
        "    # Padding o Troncamento per lunghezza fissa\n",
        "    if len(tokenized) < max_length:\n",
        "        tokenized += [0] * (max_length - len(tokenized))  # Pad con zeri\n",
        "    else:\n",
        "        tokenized = tokenized[:max_length]  # Troncamento\n",
        "\n",
        "    # Converte in tensore\n",
        "    return torch.tensor(tokenized, dtype=torch.float)\n",
        "\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, root, split, preprocess_params=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Path to the dataset directory.\n",
        "            split (str): Dataset split, either 'train' or 'test'.\n",
        "            preprocess_params (dict, optional): Parameters for running preprocess.sh script. Keys include:\n",
        "                - sharding (str): 'iid' or 'niid' for data partitioning.\n",
        "                - iu (float): Fraction of users if i.i.d. sampling.\n",
        "                - sf (float): Fraction of data to sample.\n",
        "                - k (int): Minimum number of samples per user.\n",
        "                - t (str): 'user' or 'sample' for train-test partition.\n",
        "                - tf (float): Fraction of data in training set.\n",
        "                - raw (bool): Include raw text data.\n",
        "                - smplseed (int): Seed for sampling.\n",
        "                - spltseed (int): Seed for splitting.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.preprocess_params = preprocess_params or {}\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = text_transform\n",
        "\n",
        "        # Ensure the working directory is set to the dataset folder\n",
        "        os.chdir(self.root)\n",
        "\n",
        "        # Run preprocessing script if needed\n",
        "        self._preprocess_data()\n",
        "\n",
        "        # Load the dataset\n",
        "        self.data = self._load_data()\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"Runs preprocess.sh with the given parameters.\"\"\"\n",
        "        cmd = \"bash preprocess.sh\"\n",
        "\n",
        "        if 'sharding' in self.preprocess_params:\n",
        "            cmd += f\" -s {self.preprocess_params['sharding']}\"\n",
        "        if 'iu' in self.preprocess_params:\n",
        "            cmd += f\" --iu {self.preprocess_params['iu']}\"\n",
        "        if 'sf' in self.preprocess_params:\n",
        "            cmd += f\" --sf {self.preprocess_params['sf']}\"\n",
        "        if 'k' in self.preprocess_params:\n",
        "            cmd += f\" -k {self.preprocess_params['k']}\"\n",
        "        if 't' in self.preprocess_params:\n",
        "            cmd += f\" -t {self.preprocess_params['t']}\"\n",
        "        if 'tf' in self.preprocess_params:\n",
        "            cmd += f\" --tf {self.preprocess_params['tf']}\"\n",
        "        if 'raw' in self.preprocess_params and self.preprocess_params['raw']:\n",
        "            cmd += f\" --raw\"\n",
        "        if 'smplseed' in self.preprocess_params:\n",
        "            cmd += f\" --smplseed {self.preprocess_params['smplseed']}\"\n",
        "        if 'spltseed' in self.preprocess_params:\n",
        "            cmd += f\" --spltseed {self.preprocess_params['spltseed']}\"\n",
        "\n",
        "        print(f\"Running command: {cmd}\")\n",
        "        os.system(cmd)\n",
        "        os.chdir(DIR_DATA)\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "      \"\"\"Loads data from the JSON file in the train or test folder, assuming only one file per folder.\"\"\"\n",
        "      # Identifica il file JSON nella directory specificata\n",
        "      folder_path = os.path.join(self.root,'data', self.split)\n",
        "      json_files = [f for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
        "\n",
        "      if len(json_files) != 1:\n",
        "          raise ValueError(f\"Expected exactly one JSON file in {folder_path}, but found {len(json_files)} files.\")\n",
        "\n",
        "      file_path = os.path.join(folder_path, json_files[0])\n",
        "\n",
        "      # Carica i dati dal file JSON\n",
        "      with open(file_path, 'r') as f:\n",
        "          data = json.load(f)\n",
        "\n",
        "      # Converti la struttura JSON in un DataFrame di pandas\n",
        "      records = []\n",
        "      for user, user_data in data['user_data'].items():\n",
        "          for x, y in zip(user_data['x'], user_data['y']):\n",
        "              records.append({\n",
        "                  'user': user,\n",
        "                  'input': x,\n",
        "                  'target': y\n",
        "              })\n",
        "\n",
        "      return pd.DataFrame(records)\n",
        "\n",
        "\n",
        "    def get_dataframe(self):\n",
        "        \"\"\"Returns the dataset as a pandas DataFrame.\"\"\"\n",
        "        return self.data\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'input': self.data.iloc[idx]['input'],\n",
        "            'target': self.data.iloc[idx]['target']\n",
        "        }\n",
        "\n",
        "        # Applica la trasformazione agli input (e.g., tokenizzazione e padding)\n",
        "        if self.transform:\n",
        "            sample['input'] = self.transform(sample['input'])\n",
        "\n",
        "        # Converte i target in tensori\n",
        "        sample['target'] = torch.tensor(sample['target'], dtype=torch.long)\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "KNrpQePKpJOS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shakespeare Model Architecture"
      ],
      "metadata": {
        "id": "5ASBPuONliim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers):\n",
        "        super(ShakespeareRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)  # Embed input\n",
        "        out, hidden = self.lstm(x, hidden)  # Pass through LSTM layers\n",
        "        out = self.fc(out)  # Fully connected layer for output\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "e6JWBdeWlmBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPueHaDjcwN"
      },
      "source": [
        "## Centralized training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8YKXaBSjcwP"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Carica checkpoint se esiste\n",
        "    start_epoch = load_checkpoint(model, optimizer, hyperparameters,\"Centralized/\")\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Salva checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch, hyperparameters,\"Centralized/\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, test_losses, test_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Centralized training"
      ],
      "metadata": {
        "id": "Rys0x0RL4xDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Always run before creating new datasets\n",
        "\n",
        "if os.path.exists(\"/content/leaf/\"):\n",
        "  # Use shutil.rmtree to remove the folder and its contents\n",
        "  shutil.rmtree(\"/content/leaf\")\n",
        "  print(f\"Successfully deleted folder leaf\")\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "!git clone https://github.com/maxfra01/leaf.git\n",
        "\n",
        "# -----------------------------------------\n",
        "\n",
        "preprocess_params = {\n",
        "        'sharding': 'iid',\n",
        "        'sf': 1.0,\n",
        "        't': 'sample',\n",
        "        'tf': 0.8,\n",
        "    } # Get the full-size dataset\n",
        "\n",
        "train_dataset_big = ShakespeareDataset(root=\"leaf/data/shakespeare/\", split=\"train\", preprocess_params=preprocess_params)\n",
        "test_dataset = ShakespeareDataset(root=\"leaf/data/shakespeare\", split=\"test\", preprocess_params=preprocess_params)\n"
      ],
      "metadata": {
        "id": "2qfjvLkl-_P3",
        "outputId": "8512e241-0f77-4f5c-9c8e-a59898dee12d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully deleted folder leaf\n",
            "Cloning into 'leaf'...\n",
            "remote: Enumerating objects: 772, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 772 (delta 0), reused 0 (delta 0), pack-reused 766 (from 1)\u001b[K\n",
            "Receiving objects: 100% (772/772), 6.78 MiB | 18.33 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n",
            "Running command: bash preprocess.sh -s iid --sf 1.0 -t sample --tf 0.8\n",
            "Running command: bash preprocess.sh -s iid --sf 1.0 -t sample --tf 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.8\n",
        "WEIGHT_DECAY=1e-4\n",
        "EPOCHS = 50\n",
        "\n",
        "hyperparameters = f\"BS{BATCH_SIZE}_LR{LEARNING_RATE}_WD{WEIGHT_DECAY}_M{MOMENTUM}\"\n",
        "\n",
        "\n",
        "# Create the validation split\n",
        "indexes = range(0, len(train_dataset_big))\n",
        "splitting = train_test_split(indexes, train_size = 0.8, random_state = 42, shuffle = True)\n",
        "train_indexes = splitting[0]\n",
        "val_indexes = splitting[1]\n",
        "\n",
        "train_dataset = Subset(train_dataset_big, train_indexes)\n",
        "val_dataset = Subset(train_dataset_big, val_indexes)\n",
        "\n",
        "# Create Dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Set device and model parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_dim = len(train_dataset[0]['input'])\n",
        "output_dim = len(set(train_dataset_big.data['target']))  # Numero di classi\n",
        "\n",
        "model_shakespeare = ShakespeareRNN(input_dim=input_dim, output_dim=output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_shakespeare.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "# Train the model\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "train_losses, val_losses, val_accuracies = train_model(\n",
        "    model=model_shakespeare,\n",
        "    train_loader=train_dataloader,\n",
        "    test_loader=val_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    criterion=criterion,\n",
        "    epochs=EPOCHS,\n",
        "    hyperparameters=hyperparameters\n",
        ")\n",
        "\n",
        "# Evaluation on test split\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
        "\n",
        "test_loss, test_accuracy = evaluate_model(model_shakespeare, test_dataloader, criterion, device)\n",
        "\n",
        "print(\"\\nFinal Model Evaluation on Test Set:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(val_losses, label='Shakespeare Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Shakespare Val Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bUGr07HXneev",
        "outputId": "8eef7658-a3a2-4cc7-a0c6-309acd70eb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "new(): invalid data type 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d99f68ed975c>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Set device and model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_big\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Numero di classi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_T_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-203d8d4df374>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# Converte i target in tensori\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1rnjkaAjcwQ"
      },
      "source": [
        "## Federate Learning classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJm8S7pjjcwR"
      },
      "outputs": [],
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, epochs, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "      #print(f\"Client {self.client_id}, Epoch {epoch+1}/{epochs}\")\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n",
        "\n",
        "class Server:\n",
        "  def __init__(self, model, clients, test_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "\n",
        "  def federated_averaging(self, epochs, batch_size, num_rounds, fraction_fit, skewness=None, hyperparameters = None, fedOptimizer=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "\n",
        "    # Carica il checkpoint se esiste\n",
        "    if skewness is  None:\n",
        "      start_epoch = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "    else:\n",
        "      start_epoch = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Skewed/\")\n",
        "\n",
        "\n",
        "    # Initialize variables for FedOptimizers\n",
        "    if fedOptimizer in {\"FedAdaGrad\", \"FedYogi\", \"FedAdam\"}:\n",
        "        optimizer_state = {\n",
        "            \"m\": {key: torch.zeros_like(value, dtype=torch.float32) for key, value in self.model.state_dict().items()},\n",
        "            \"v\": {key: torch.zeros_like(value, dtype=torch.float32) for key, value in self.model.state_dict().items()},\n",
        "        }\n",
        "        beta1 = 0.9  # Momentum parameter for Adam-based optimizers\n",
        "        beta2 = 0.999  # RMS parameter for Adam-based optimizers\n",
        "        lr = 0.01  # Learning rate\n",
        "        eps = 1e-8  # Small constant to prevent division by zero\n",
        "\n",
        "    for round in range(start_epoch, num_rounds):\n",
        "        print(f\"Round {round + 1}/{num_rounds}\")\n",
        "\n",
        "        if skewness is not None:\n",
        "            probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "            selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit * len(self.clients))),\n",
        "                                                replace=False, p=probabilities)\n",
        "        else:\n",
        "            selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit * len(self.clients))),\n",
        "                                                replace=False)\n",
        "\n",
        "        global_weights = self.model.state_dict()\n",
        "\n",
        "        # Simulate parallel client training\n",
        "        client_weights = {}\n",
        "        for client in selected_clients:\n",
        "            client_weights[client.client_id] = client.train(global_weights, epochs, batch_size)\n",
        "\n",
        "        # Aggregate client updates\n",
        "        total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "        aggregated_updates = {key: torch.zeros_like(value, dtype=torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "        for client in selected_clients:\n",
        "            scaling_factor = len(client.data) / total_data_size\n",
        "            for key in aggregated_updates.keys():\n",
        "                aggregated_updates[key] += scaling_factor * (client_weights[client.client_id][key] - global_weights[key])\n",
        "\n",
        "        # Apply selected FedOptimizer\n",
        "        if fedOptimizer == \"FedAdaGrad\":\n",
        "            for key in global_weights.keys():\n",
        "                optimizer_state[\"v\"][key] += aggregated_updates[key] ** 2\n",
        "                global_weights[key] += lr * aggregated_updates[key] / (torch.sqrt(optimizer_state[\"v\"][key]) + eps)\n",
        "\n",
        "        elif fedOptimizer == \"FedYogi\":\n",
        "            for key in global_weights.keys():\n",
        "                optimizer_state[\"v\"][key] -= (1 - beta2) * aggregated_updates[key] ** 2 * torch.sign(\n",
        "                    optimizer_state[\"v\"][key] - aggregated_updates[key] ** 2)\n",
        "                global_weights[key] += lr * aggregated_updates[key] / (torch.sqrt(optimizer_state[\"v\"][key]) + eps)\n",
        "\n",
        "        elif fedOptimizer == \"FedAdam\":\n",
        "            for key in global_weights.keys():\n",
        "                optimizer_state[\"m\"][key] = beta1 * optimizer_state[\"m\"][key] + (1 - beta1) * aggregated_updates[key]\n",
        "                optimizer_state[\"v\"][key] = beta2 * optimizer_state[\"v\"][key] + (1 - beta2) * aggregated_updates[key] ** 2\n",
        "                m_hat = optimizer_state[\"m\"][key] / (1 - beta1 ** (round + 1))\n",
        "                v_hat = optimizer_state[\"v\"][key] / (1 - beta2 ** (round + 1))\n",
        "                global_weights[key] += lr * m_hat / (torch.sqrt(v_hat) + eps)\n",
        "\n",
        "        else:  # Default to FedAvg\n",
        "            for key in global_weights.keys():\n",
        "                global_weights[key] += aggregated_updates[key]\n",
        "\n",
        "        # Update global model weights\n",
        "        self.model.load_state_dict(global_weights)\n",
        "\n",
        "        if skewness is  None:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\")\n",
        "        else:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Skewed/\")\n",
        "\n",
        "\n",
        "        # Evaluate global model\n",
        "        loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True),\n",
        "                                        nn.CrossEntropyLoss(), device)\n",
        "        self.round_losses.append(loss)\n",
        "        self.round_accuracies.append(accuracy)\n",
        "        print(f\"Round {round + 1}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='Test Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solver"
      ],
      "metadata": {
        "id": "QDWAowUBmxID"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ypHP_8fmyl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}