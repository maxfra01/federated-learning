{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK5E-uqb92ci"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LVs00uLvecJo"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvuWze6kjxhV"
      },
      "source": [
        "## Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UihnNmjPjxhV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "\n",
        "DIR_DATA = \"../data\"\n",
        "CHECKPOINT_DIR = '../checkpoints'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", data_to_save=None):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -10}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "    previous_filename_json = f\"model_epoch_{epoch -10}_params_{hyperparameters}.json\"\n",
        "    previous_filepath_json = os.path.join(subfolder_path, previous_filename_json)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath) and os.path.exists(previous_filepath_json):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "    with open(filepath_json, 'w') as json_file:\n",
        "      json.dump(data_to_save, json_file, indent=4)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1, None  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca pi√π alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Trova e carica il file JSON associato\n",
        "        json_filename = latest_file.replace('.pth', '.json')\n",
        "        json_filepath = os.path.join(subfolder_path, json_filename)\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(f\"JSON data loaded: {json_filepath}\")\n",
        "        else:\n",
        "            print(f\"No JSON file found for: {latest_file}\")\n",
        "\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1, None  # Le epoche iniziano da 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBqh7noxfUWv"
      },
      "source": [
        "# CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbl-Cn0aw2CW"
      },
      "outputs": [],
      "source": [
        "class CIFAR100Dataset(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None, sharding=None, K=10, Nc=2):\n",
        "        \"\"\"\n",
        "        CIFAR-100 Dataset with IID and non-IID sharding.\n",
        "\n",
        "        Args:\n",
        "        - root (str): Directory to store the dataset.\n",
        "        - split (str): 'train' or 'test'.\n",
        "        - transform (callable): Transformations applied to the images.\n",
        "        - sharding (str): 'iid' or 'niid'.\n",
        "        - K (int): Number of clients for the sharding.\n",
        "        - Nc (int): Number of classes per client (used for non-iid sharding).\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.sharding = sharding\n",
        "        self.K = K\n",
        "        self.Nc = Nc\n",
        "\n",
        "\n",
        "        # Default transformations if none are provided\n",
        "        if self.transform is None:\n",
        "            if self.split == 'train':\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.RandomHorizontalFlip(),  # Flip orizzontale casuale\n",
        "                    transforms.RandomRotation(10),\n",
        "                    transforms.ToTensor(),  # Converte l'immagine in un tensore PyTorch\n",
        "                    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # Normalizzazione\n",
        "                ])\n",
        "            else:\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),  # Converte in tensore PyTorch\n",
        "                    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # Normalizzazione\n",
        "                ])\n",
        "\n",
        "        dataset = datasets.CIFAR100(\n",
        "            root=self.root,\n",
        "            train=(self.split == 'train'),\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.data = pd.DataFrame({\n",
        "            \"image\": [dataset[i][0] for i in range(len(dataset))],\n",
        "            \"label\": [dataset[i][1] for i in range(len(dataset))]\n",
        "        })\n",
        "\n",
        "        if self.split == 'train' and self.sharding:\n",
        "            self.data = self._apply_sharding()\n",
        "\n",
        "    def _apply_sharding(self):\n",
        "        \"\"\"Apply IID or non-IID sharding to the training data.\"\"\"\n",
        "        if self.sharding == 'iid':\n",
        "            return self._iid_sharding()\n",
        "        elif self.sharding == 'niid':\n",
        "            return self._non_iid_sharding()\n",
        "        else:\n",
        "            raise ValueError(\"Sharding must be 'iid' or 'niid'.\")\n",
        "\n",
        "    def _iid_sharding(self):\n",
        "        \"\"\"Split data IID: uniformly distribute samples across K clients.\"\"\"\n",
        "        data_split = []\n",
        "        indices = self.data.index.tolist()\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        # Split indices equally among K clients\n",
        "        client_indices = [indices[i::self.K] for i in range(self.K)]\n",
        "\n",
        "        for client_id, idxs in enumerate(client_indices):\n",
        "            client_data = self.data.loc[idxs].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def _non_iid_sharding(self):\n",
        "        \"\"\"Non-IID sharding with fixed number of classes per client\"\"\"\n",
        "        data_split = []\n",
        "        labels = self.data['label'].unique()\n",
        "        samples_per_client = len(self.data) // self.K\n",
        "\n",
        "        for client_id in range(self.K):\n",
        "            # Seleziona Nc classi casuali per questo client\n",
        "            client_classes = np.random.choice(labels, size=self.Nc, replace=False)\n",
        "\n",
        "            # Ottieni i dati per le classi selezionate\n",
        "            client_data = pd.DataFrame()\n",
        "            samples_per_class = samples_per_client // self.Nc\n",
        "\n",
        "            #print(f\"Client {client_id}:\")\n",
        "            #print(f\"Classi assegnate: {client_classes}\")\n",
        "\n",
        "            for class_ in client_classes:\n",
        "                class_data = self.data[self.data['label'] == class_]\n",
        "                samples = class_data.sample(n=samples_per_class, replace=True)\n",
        "                client_data = pd.concat([client_data, samples])\n",
        "\n",
        "                #print(f\"  Classe {class_}: {len(samples)} campioni\")\n",
        "\n",
        "            client_data['client_id'] = client_id\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        # Concatenate tutti i dati per avere il dataset completo per tutti i clienti\n",
        "        all_data = pd.concat(data_split, ignore_index=True)\n",
        "        return all_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        image, label = row['image'], row['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ptw5_Q9Yv4u"
      },
      "source": [
        "## LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iekAEyoGYx0p"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self,num_classes=100):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64 * 5 * 5, 384),  # Updated to be consistent with data augmentation\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 192),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(192, num_classes)  # 100 classes for CIFAR-100\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output of the conv layers\n",
        "        x = self.fc_layer(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdRUiDbHz-ob"
      },
      "source": [
        "# Centralized training of CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHgLtGILz8uS"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, validation_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, validation_losses, validation_accuracies, test_losses, test_accuracies = [], [], [], [], []\n",
        "\n",
        "    # Carica checkpoint se esiste\n",
        "    start_epoch, json_data = load_checkpoint(model, optimizer, hyperparameters, \"Centralized/\")\n",
        "    if json_data is not None:\n",
        "        validation_losses = json_data.get('validation_losses', [])\n",
        "        validation_accuracies = json_data.get('validation_accuracies', [])\n",
        "        train_losses = json_data.get('train_losses', [])\n",
        "        test_losses = json_data.get('test_losses', [])\n",
        "        test_accuracies = json_data.get('test_accuracies', [])\n",
        "\n",
        "    if start_epoch >= epochs:\n",
        "        print(f\"Checkpoint trovato, configurazione gi√† completata. Valutazione solo sul validation set.\")\n",
        "        validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "        return train_losses, validation_losses, validation_accuracies, test_losses, test_accuracies\n",
        "\n",
        "    for epoch in range(start_epoch, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Valutazione sul validation set\n",
        "        validation_loss, validation_accuracy = evaluate_model(model, validation_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        validation_losses.append(validation_loss)\n",
        "        validation_accuracies.append(validation_accuracy)\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        # Salva checkpoint\n",
        "        save_checkpoint(\n",
        "            model, optimizer, epoch, hyperparameters, \"Centralized/\",\n",
        "            data_to_save={\n",
        "                'validation_losses': validation_losses,\n",
        "                'validation_accuracies': validation_accuracies,\n",
        "                'test_losses': test_losses,\n",
        "                'test_accuracies': test_accuracies,\n",
        "                'train_losses': train_losses\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, validation_losses, validation_accuracies, test_losses, test_accuracies\n",
        "\n",
        "\n",
        "def evaluate_model(model, validation_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in validation_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(validation_loader), correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1JKWoaq7ODR"
      },
      "outputs": [],
      "source": [
        "# Configurazione iperparametri migliori\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.001\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # Normalizzazione\n",
        "])\n",
        "\n",
        "train_dataset = CIFAR100Dataset(DIR_DATA, split='train', transform=transform_train)\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "train_indices, validation_indices = train_test_split(\n",
        "    range(len(train_dataset)), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Crea Subset\n",
        "train_dataset = Subset(train_dataset, train_indices)\n",
        "validation_dataset = Subset(CIFAR100Dataset(DIR_DATA, split='train'), validation_indices)\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "model_cifar = LeNet5(100)\n",
        "optimizer_cifar = optim.SGD(model_cifar.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler_cifar = CosineAnnealingLR(optimizer_cifar, T_max=EPOCHS)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training del modello\n",
        "train_losses, validation_losses, validation_accuracies, test_losses, test_accuracies = train_model(\n",
        "    model=model_cifar,\n",
        "    train_loader=train_dataloader,\n",
        "    validation_loader=validation_dataloader,\n",
        "    test_loader=test_dataloader,\n",
        "    optimizer=optimizer_cifar,\n",
        "    scheduler=scheduler_cifar,\n",
        "    criterion=criterion,\n",
        "    epochs=EPOCHS,\n",
        "    hyperparameters=f\"BS{BATCH_SIZE}_LR{LEARNING_RATE}_WD{WEIGHT_DECAY}_M{MOMENTUM}\"\n",
        ")\n",
        "\n",
        "# Grafici\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Validation Loss\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='Validation Loss', color='blue')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Validation Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.savefig('validation_loss.png')\n",
        "\n",
        "# Validation Accuracy\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(range(1, len(validation_accuracies) + 1), validation_accuracies, label='Validation Accuracy', color='green')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy per Epoch')\n",
        "plt.legend()\n",
        "plt.savefig('validation_accuracy.png')\n",
        "\n",
        "# Test Loss\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss', color='orange')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.savefig('test_loss.png')\n",
        "\n",
        "# Test Accuracy\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Test Accuracy', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy per Epoch')\n",
        "plt.legend()\n",
        "plt.savefig('test_accuracy.png')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgqkHxRbdJK-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCdLhW1Vv80"
      },
      "source": [
        "## FL Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxpBSJyCoSPx"
      },
      "outputs": [],
      "source": [
        "def plot_selected_clients_distribution(selected_clients_per_round, num_clients, hyperparameters):\n",
        "    \"\"\"Plotta la distribuzione dei client selezionati alla fine del processo.\"\"\"\n",
        "    counts = np.zeros(num_clients)\n",
        "\n",
        "    # Conta quante volte ogni client √® stato selezionato in tutti i round\n",
        "    for selected_clients in selected_clients_per_round:\n",
        "        for client in selected_clients:\n",
        "            counts[client] += 1\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(num_clients), counts, color='skyblue', edgecolor='black')\n",
        "    plt.title(\"Distribuzione dei Client Selezionati Durante il Federated Averaging\")\n",
        "    plt.xlabel(\"Client ID\")\n",
        "    plt.ylabel(\"Frequenza di Selezione\")\n",
        "    plt.grid(axis='y')\n",
        "    plt.savefig(f\"CIFAR100_Client_distribution_{hyperparameters}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, local_steps, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
        "\n",
        "    steps = 0\n",
        "    while steps < local_steps:\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "        if steps >= local_steps:\n",
        "          break\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkSMDm61Acaf"
      },
      "outputs": [],
      "source": [
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data, val_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.val_data = val_data\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "    self.selected_clients_per_round = [] #clint selezionati per skewness\n",
        "    self.test_losses = []\n",
        "    self.test_accuracies = []\n",
        "\n",
        "  def generate_skewed_probabilities(num_clients, gamma=0.5):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet(np.ones(num_clients) * gamma)\n",
        "    return probabilities\n",
        "\n",
        "  def client_selection(self, num_clients, fraction, probabilities=None):\n",
        "      \"It selects a subset of clients based on uniform or skewed distribution\"\n",
        "      num_clients_to_select = int(num_clients * fraction)\n",
        "      if probabilities is None:\n",
        "          selected_clients = np.random.choice(num_clients, num_clients_to_select, replace=False)\n",
        "      else\n",
        "          selected_clients = np.random.choice(num_clients, num_clients_to_select, p=probabilities, replace=False)\n",
        "      return selected_clients\n",
        "\n",
        "  def federated_averaging(self, local_steps, batch_size, num_rounds, fraction_fit, skewness = None, hyperparameters = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "     # Carica il checkpoint se esiste\n",
        "    data_to_load = None\n",
        "    if skewness is  None:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "    else:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Skewed/\")\n",
        "\n",
        "    if data_to_load is not None:\n",
        "      self.round_losses = data_to_load['round_losses']\n",
        "      self.round_accuracies = data_to_load['round_accuracies']\n",
        "      self.selected_clients_per_round = data_to_load['selected_clients_per_round']\n",
        "      self.test_losses = data_to_load['test_losses']\n",
        "      self.test_accuracies = data_to_load['test_accuracies']\n",
        "\n",
        "    probabilities = None\n",
        "    if skewness is not None:\n",
        "      probabilities = self.generate_skewed_probabilities(len(self.clients), skewness)\n",
        "\n",
        "    for round in range(start_epoch, num_rounds+1):\n",
        "\n",
        "      selected_clients = self.client_selection(len(self.clients), fraction_fit, probabilities)\n",
        "\n",
        "      self.selected_clients_per_round.append([client.client_id for client in selected_clients])\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, local_steps, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model every 10 rounds\n",
        "      if round % 10 == 0:\n",
        "        loss, accuracy = evaluate_model(self.model, DataLoader(self.val_data, batch_size=batch_size, shuffle=False, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "        loss_test, accuracy_test = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=False, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "\n",
        "        self.round_losses.append(loss)\n",
        "        self.round_accuracies.append(accuracy)\n",
        "        self.test_losses.append(loss_test)\n",
        "        self.test_accuracies.append(accuracy_test)\n",
        "        print(f\"Round {round}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        data_to_save = {\n",
        "          'round_losses': self.round_losses,\n",
        "          'round_accuracies': self.round_accuracies,\n",
        "          'test_losses': self.test_losses,\n",
        "          'test_accuracies': self.test_accuracies,\n",
        "          'selected_clients_per_round': [[client for client in round_clients] for round_clients in self.selected_clients_per_round]  # Serializziamo solo i client_id\n",
        "      }\n",
        "\n",
        "        if skewness is  None:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\", data_to_save)\n",
        "        else:\n",
        "          save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Skewed/\", data_to_save)\n",
        "\n",
        "\n",
        "\n",
        "    #print(\"Evaluation on test set...\")\n",
        "    #loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=False, pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "    #print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "     # Plot dei risultati\n",
        "    plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Validation Loss\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(range(0, num_rounds, 10), self.round_losses, label='Validation Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss per Round')\n",
        "    plt.legend()\n",
        "\n",
        "        # Validation Accuracy\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(range(0, num_rounds, 10), self.round_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Accuracy per Round')\n",
        "    plt.legend()\n",
        "\n",
        "    # Test Loss\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(range(0, num_rounds, 10), self.test_losses, label='Test Loss', color='orange')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss per Round')\n",
        "    plt.legend()\n",
        "\n",
        "    # Test Accuracy\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(range(0, num_rounds, 10), self.test_accuracies, label='Test Accuracy', color='green')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy per Round')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    file_name = f\"CIFAR100_fedavg_uniform_{hyperparameters}.jpg\" if skewness is None else f\"CIFAR100_fedavg_skew_{hyperparameters}.jpg\"\n",
        "    plt.savefig(file_name)\n",
        "    plt.show()\n",
        "\n",
        "    plot_selected_clients_distribution(self.selected_clients_per_round, len(self.clients), hyperparameters)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nEh36xySq9Pr",
        "outputId": "f560db5a-58aa-45ad-e9f2-fc754ea70c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "No checkpoint found, Starting now...\n",
            "Round 10/2000 - Loss: 4.4581, Accuracy: 0.0400\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_10_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 20/2000 - Loss: 4.0582, Accuracy: 0.0813\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_20_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 30/2000 - Loss: 3.9466, Accuracy: 0.0936\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_30_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 40/2000 - Loss: 3.7234, Accuracy: 0.1347\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_40_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 50/2000 - Loss: 3.6019, Accuracy: 0.1537\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_50_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 60/2000 - Loss: 3.5398, Accuracy: 0.1610\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_60_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 70/2000 - Loss: 3.3709, Accuracy: 0.1919\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_70_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 80/2000 - Loss: 3.2963, Accuracy: 0.2104\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_80_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 90/2000 - Loss: 3.3024, Accuracy: 0.2017\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_90_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 100/2000 - Loss: 3.2192, Accuracy: 0.2213\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_100_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 110/2000 - Loss: 3.1110, Accuracy: 0.2432\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_110_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 120/2000 - Loss: 3.0909, Accuracy: 0.2470\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_120_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 130/2000 - Loss: 3.0086, Accuracy: 0.2539\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_130_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 140/2000 - Loss: 2.9513, Accuracy: 0.2767\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_140_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 150/2000 - Loss: 3.0000, Accuracy: 0.2628\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_150_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 160/2000 - Loss: 2.9365, Accuracy: 0.2812\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_160_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 170/2000 - Loss: 2.8568, Accuracy: 0.2922\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_170_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 180/2000 - Loss: 2.8263, Accuracy: 0.2943\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_180_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 190/2000 - Loss: 2.7426, Accuracy: 0.3118\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_190_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 200/2000 - Loss: 2.7360, Accuracy: 0.3172\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_200_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 210/2000 - Loss: 2.7535, Accuracy: 0.3205\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_210_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 220/2000 - Loss: 2.7461, Accuracy: 0.3239\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_220_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 230/2000 - Loss: 2.6876, Accuracy: 0.3292\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_230_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 240/2000 - Loss: 2.7031, Accuracy: 0.3308\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_240_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 250/2000 - Loss: 2.6471, Accuracy: 0.3426\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_250_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 260/2000 - Loss: 2.6427, Accuracy: 0.3456\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_260_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 270/2000 - Loss: 2.7062, Accuracy: 0.3336\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_270_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 280/2000 - Loss: 2.6742, Accuracy: 0.3376\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_280_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 290/2000 - Loss: 2.6308, Accuracy: 0.3445\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_290_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 300/2000 - Loss: 2.6391, Accuracy: 0.3529\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_300_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 310/2000 - Loss: 2.6993, Accuracy: 0.3445\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_310_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 320/2000 - Loss: 2.5589, Accuracy: 0.3607\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_320_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 330/2000 - Loss: 2.5808, Accuracy: 0.3598\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_330_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 340/2000 - Loss: 2.5744, Accuracy: 0.3652\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_340_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 350/2000 - Loss: 2.5616, Accuracy: 0.3670\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_350_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 360/2000 - Loss: 2.6089, Accuracy: 0.3662\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_360_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 370/2000 - Loss: 2.4732, Accuracy: 0.3879\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_370_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 380/2000 - Loss: 2.6159, Accuracy: 0.3650\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_380_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 390/2000 - Loss: 2.5354, Accuracy: 0.3784\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_390_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 400/2000 - Loss: 2.5802, Accuracy: 0.3793\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_400_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 410/2000 - Loss: 2.5727, Accuracy: 0.3805\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_410_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 420/2000 - Loss: 2.5426, Accuracy: 0.3832\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_420_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 430/2000 - Loss: 2.5875, Accuracy: 0.3798\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_430_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 440/2000 - Loss: 2.6098, Accuracy: 0.3675\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_440_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 450/2000 - Loss: 2.5423, Accuracy: 0.3853\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_450_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 460/2000 - Loss: 2.5777, Accuracy: 0.3827\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_460_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 470/2000 - Loss: 2.5440, Accuracy: 0.3996\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_470_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 480/2000 - Loss: 2.7292, Accuracy: 0.3776\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_480_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 490/2000 - Loss: 2.5564, Accuracy: 0.3873\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_490_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 500/2000 - Loss: 2.6106, Accuracy: 0.3941\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_500_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 510/2000 - Loss: 2.6987, Accuracy: 0.3880\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_510_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 520/2000 - Loss: 2.6455, Accuracy: 0.3878\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_520_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 530/2000 - Loss: 2.6767, Accuracy: 0.3767\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_530_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 540/2000 - Loss: 2.6883, Accuracy: 0.3886\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_540_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 550/2000 - Loss: 2.6724, Accuracy: 0.3880\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_550_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 560/2000 - Loss: 2.6678, Accuracy: 0.3869\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_560_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 570/2000 - Loss: 2.7138, Accuracy: 0.3915\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_570_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 580/2000 - Loss: 2.6996, Accuracy: 0.3948\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_580_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 590/2000 - Loss: 2.6885, Accuracy: 0.3867\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_590_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 600/2000 - Loss: 2.8252, Accuracy: 0.3845\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_600_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 610/2000 - Loss: 2.7855, Accuracy: 0.3889\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_610_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 620/2000 - Loss: 2.7891, Accuracy: 0.3868\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_620_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 630/2000 - Loss: 2.8137, Accuracy: 0.3901\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_630_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 640/2000 - Loss: 2.8466, Accuracy: 0.3843\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_640_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 650/2000 - Loss: 2.7875, Accuracy: 0.4006\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_650_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 660/2000 - Loss: 2.8813, Accuracy: 0.3969\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_660_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 670/2000 - Loss: 2.9524, Accuracy: 0.3904\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_670_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 680/2000 - Loss: 2.9581, Accuracy: 0.3889\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_680_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 690/2000 - Loss: 2.9824, Accuracy: 0.3887\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_690_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 700/2000 - Loss: 2.9301, Accuracy: 0.3862\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_700_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 710/2000 - Loss: 2.9378, Accuracy: 0.3917\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_710_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 720/2000 - Loss: 2.9368, Accuracy: 0.3926\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_720_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 730/2000 - Loss: 2.8967, Accuracy: 0.3891\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_730_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 740/2000 - Loss: 2.9736, Accuracy: 0.3925\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_740_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 750/2000 - Loss: 3.0358, Accuracy: 0.3906\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_750_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 760/2000 - Loss: 3.1174, Accuracy: 0.3891\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_760_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 770/2000 - Loss: 3.0503, Accuracy: 0.4013\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_770_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 780/2000 - Loss: 3.1873, Accuracy: 0.3741\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_780_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 790/2000 - Loss: 3.1956, Accuracy: 0.3834\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_790_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 800/2000 - Loss: 3.0397, Accuracy: 0.3892\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_800_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 810/2000 - Loss: 3.1416, Accuracy: 0.3821\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_810_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 820/2000 - Loss: 3.1247, Accuracy: 0.3957\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_820_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 830/2000 - Loss: 3.1671, Accuracy: 0.3921\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_830_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 840/2000 - Loss: 3.1329, Accuracy: 0.3942\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_840_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 850/2000 - Loss: 3.3036, Accuracy: 0.3696\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_850_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 860/2000 - Loss: 3.2559, Accuracy: 0.3956\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_860_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 870/2000 - Loss: 3.2768, Accuracy: 0.3756\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_870_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 880/2000 - Loss: 3.2075, Accuracy: 0.3863\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_880_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 890/2000 - Loss: 3.2695, Accuracy: 0.3852\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_890_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 900/2000 - Loss: 3.2375, Accuracy: 0.3921\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_900_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 910/2000 - Loss: 3.2841, Accuracy: 0.3900\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_910_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 920/2000 - Loss: 3.3567, Accuracy: 0.3796\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_920_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 930/2000 - Loss: 3.3240, Accuracy: 0.3885\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_930_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 940/2000 - Loss: 3.2773, Accuracy: 0.3820\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_940_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 950/2000 - Loss: 3.3865, Accuracy: 0.3958\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_950_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 960/2000 - Loss: 3.5098, Accuracy: 0.3853\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_960_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 970/2000 - Loss: 3.3814, Accuracy: 0.3926\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_970_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 980/2000 - Loss: 3.3839, Accuracy: 0.3910\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_980_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 990/2000 - Loss: 3.4712, Accuracy: 0.3853\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_990_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 1000/2000 - Loss: 3.4225, Accuracy: 0.3879\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_1000_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 1010/2000 - Loss: 3.4635, Accuracy: 0.3861\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_1010_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 1020/2000 - Loss: 3.4992, Accuracy: 0.3891\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_1020_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n",
            "Round 1030/2000 - Loss: 3.4561, Accuracy: 0.3897\n",
            "Checkpoint salvato: ../checkpoints\\Federated_Uniform/model_epoch_1030_params_BS64_LR0.01_M0.9_WD0.001_J4_C0.1.pth\n"
          ]
        }
      ],
      "source": [
        "K = 100 #fix\n",
        "LOCAL_STEPS = 4 # J\n",
        "ROUNDS = 2000\n",
        "C = 0.1 #fix\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-3\n",
        "SKEWNESS=0.01\n",
        "\n",
        "optimizer_params = {\n",
        "      \"lr\": LR,\n",
        "      \"momentum\": MOMENTUM,\n",
        "      \"weight_decay\": WEIGHT_DECAY\n",
        "  }\n",
        "\n",
        "model_cifar = LeNet5(100)\n",
        "\n",
        "train_dataset_big = CIFAR100Dataset(DIR_DATA, split=\"train\", sharding=\"iid\", K=K)\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split=\"test\")\n",
        "\n",
        "# Split training-validation\n",
        "train_indices, validation_indices = train_test_split(\n",
        "    range(len(train_dataset_big)), test_size=0.2, random_state=42,stratify=train_dataset_big.data[\"label\"]\n",
        ")\n",
        "\n",
        "# Subset di training e validazione\n",
        "train_dataset = Subset(train_dataset_big, train_indices)\n",
        "validation_dataset = Subset(train_dataset_big, validation_indices)\n",
        "\n",
        "# Mapping degli indici\n",
        "original_to_subset = {original_idx: subset_idx for subset_idx, original_idx in enumerate(train_indices)}\n",
        "\n",
        "# Creazione dei client\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    # Filtra gli indici dei client dal dataset originale\n",
        "    client_original_indices = train_dataset_big.data[\n",
        "        train_dataset_big.data[\"client_id\"] == i\n",
        "    ].index\n",
        "\n",
        "    # Converte gli indici originali in indici del subset\n",
        "    client_subset_indices = [original_to_subset[idx] for idx in client_original_indices if idx in original_to_subset]\n",
        "\n",
        "    # Crea il subset per il client\n",
        "    client_data = Subset(train_dataset, client_subset_indices)\n",
        "    clients.append(Client(model_cifar, i, client_data, optimizer_params))\n",
        "\n",
        "\n",
        "\n",
        "server_uniform = Server(model_cifar, clients, test_dataset, validation_dataset)\n",
        "hyperparameters = f\"BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_STEPS}_C{C}\"\n",
        "server_uniform.federated_averaging(local_steps=LOCAL_STEPS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C,hyperparameters=hyperparameters)\n",
        "\n",
        "\n",
        "\n",
        "#server_skewed = Server(model_cifar, clients, test_dataset)\n",
        "#hyperparameters = f\"BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_STEPS}_C{C}_SK{SKEWNESS}\"\n",
        "#server_skewed.federated_averaging(local_steps=LOCAL_STEPS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C, skewness=SKEWNESS, hyperparameters=hyperparameters)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}