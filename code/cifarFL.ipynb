{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK5E-uqb92ci"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LVs00uLvecJo"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Google Colab specific imports\n",
        "\n",
        "DIR_DATA=\"./data\"\n",
        "CHECKPOINT_DIR = './checkpoints/'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvuWze6kjxhV"
      },
      "source": [
        "## Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UihnNmjPjxhV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "DIR_DATA = \"./data\"\n",
        "CHECKPOINT_DIR = './checkpoints/'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -1}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath):\n",
        "        os.remove(previous_filepath)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca più alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1  # Le epoche iniziano da 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBqh7noxfUWv"
      },
      "source": [
        "# CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zbl-Cn0aw2CW"
      },
      "outputs": [],
      "source": [
        "class CIFAR100Dataset(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None, sharding=None, K=10, Nc=2):\n",
        "        \"\"\"\n",
        "        CIFAR-100 Dataset with IID and non-IID sharding.\n",
        "\n",
        "        Args:\n",
        "        - root (str): Directory to store the dataset.\n",
        "        - split (str): 'train' or 'test'.\n",
        "        - transform (callable): Transformations applied to the images.\n",
        "        - sharding (str): 'iid' or 'niid'.\n",
        "        - K (int): Number of clients for the sharding.\n",
        "        - Nc (int): Number of classes per client (used for non-iid sharding).\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.sharding = sharding\n",
        "        self.K = K\n",
        "        self.Nc = Nc\n",
        "\n",
        "        # Default transformations if none are provided\n",
        "        if self.transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # CIFAR-100 normalization\n",
        "        ])\n",
        "\n",
        "        dataset = datasets.CIFAR100(\n",
        "            root=self.root,\n",
        "            train=(self.split == 'train'),\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.data = pd.DataFrame({\n",
        "            \"image\": [dataset[i][0] for i in range(len(dataset))],\n",
        "            \"label\": [dataset[i][1] for i in range(len(dataset))]\n",
        "        })\n",
        "\n",
        "        if self.split == 'train' and self.sharding:\n",
        "            self.data = self._apply_sharding()\n",
        "\n",
        "    def _apply_sharding(self):\n",
        "        \"\"\"Apply IID or non-IID sharding to the training data.\"\"\"\n",
        "        if self.sharding == 'iid':\n",
        "            return self._iid_sharding()\n",
        "        elif self.sharding == 'niid':\n",
        "            return self._non_iid_sharding()\n",
        "        else:\n",
        "            raise ValueError(\"Sharding must be 'iid' or 'niid'.\")\n",
        "\n",
        "    def _iid_sharding(self):\n",
        "        \"\"\"Split data IID: uniformly distribute samples across K clients.\"\"\"\n",
        "        data_split = []\n",
        "        indices = self.data.index.tolist()\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        # Split indices equally among K clients\n",
        "        client_indices = [indices[i::self.K] for i in range(self.K)]\n",
        "\n",
        "        for client_id, idxs in enumerate(client_indices):\n",
        "            client_data = self.data.loc[idxs].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def _non_iid_sharding(self):\n",
        "        \"\"\"Split data non-IID: assign Nc classes per client.\"\"\"\n",
        "        data_split = []\n",
        "        unique_classes = self.data['label'].unique()\n",
        "        random.shuffle(unique_classes)\n",
        "\n",
        "        # Divide classes into groups of Nc\n",
        "        class_groups = [unique_classes[i:i + self.Nc] for i in range(0, len(unique_classes), self.Nc)]\n",
        "        class_groups = class_groups[:self.K]  # Limit to K clients\n",
        "\n",
        "        for client_id, class_group in enumerate(class_groups):\n",
        "            client_data = self.data[self.data['label'].isin(class_group)].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "\n",
        "            # Ensure approximately equal samples per client\n",
        "            client_data = client_data.sample(n=len(self.data) // self.K, replace=True, random_state=42)\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        image, label = row['image'], row['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ptw5_Q9Yv4u"
      },
      "source": [
        "## LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iekAEyoGYx0p"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=0)  # 3 input channels, 64 output channels\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=0)  # 64 input channels, 64 output channels\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(1600, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with activation and pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten the tensor\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers with activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdRUiDbHz-ob"
      },
      "source": [
        "# Centralized training of CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RHgLtGILz8uS"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs,hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    # Carica checkpoint se esiste\n",
        "    start_epoch = load_checkpoint(model, optimizer, hyperparameters,\"Centralized/\")\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Salva checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch, hyperparameters,\"Centralized/\")\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, test_losses, test_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m1JKWoaq7ODR",
        "outputId": "3da3df6d-0684-4273-f0c6-be181747e453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:03<00:00, 49.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Testing configuration: LR=0.01, Momentum=0.8, Weight Decay=0.001, Batch Size=32\n",
            "Nessun checkpoint trovato, inizio da capo.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-743124eb1adf>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 train_losses_cifar, val_losses_cifar, val_accuracies_cifar = train_model(\n\u001b[0m\u001b[1;32m     47\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_cifar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-96a58c4c50bd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-74986eda7b67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "#BATCH_SIZE = 64\n",
        "#LEARNING_RATE = 0.01\n",
        "#MOMENTUM = 0.9\n",
        "#WEIGHT_DECAY = 1e-3\n",
        "EPOCHS = 50\n",
        "#hyperparameters = f\"BS{BATCH_SIZE}_LR{LEARNING_RATE}_WD{WEIGHT_DECAY}\"\n",
        "\n",
        "LEARNING_RATES = [0.01, 0.05, 0.1]\n",
        "MOMENTUMS = [0.8, 0.9, 0.99]\n",
        "WEIGHT_DECAYS = [1e-3, 1e-4, 1e-5]\n",
        "BATCH_SIZES = [32, 64, 128]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n",
        "best_train_loss = float('inf')\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "train_dataset_big = CIFAR100Dataset(DIR_DATA, split='train')\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "\n",
        "indexes = range(0, len(train_dataset_big))\n",
        "splitting = train_test_split(indexes, train_size = 0.8, random_state = 42, stratify = train_dataset_big.data[\"label\"], shuffle = True)\n",
        "train_indexes = splitting[0]\n",
        "val_indexes = splitting[1]\n",
        "\n",
        "train_dataset = Subset(train_dataset_big, train_indexes)\n",
        "val_dataset = Subset(train_dataset_big, val_indexes)\n",
        "\n",
        "for lr in LEARNING_RATES:\n",
        "    for momentum in MOMENTUMS:\n",
        "        for weight_decay in WEIGHT_DECAYS:\n",
        "            for batch_size in BATCH_SIZES:\n",
        "                hyperparameters = f\"BS{batch_size}_LR{lr}_WD{weight_decay}\"\n",
        "                print(f\"Testing configuration: LR={lr}, Momentum={momentum}, Weight Decay={weight_decay}, Batch Size={batch_size}\")\n",
        "\n",
        "                train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
        "                val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "                #test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "                model_cifar = LeNet5(100)\n",
        "\n",
        "                optimizer_cifar = optim.SGD(model_cifar.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "                scheduler_cifar = CosineAnnealingLR(optimizer_cifar, T_max=200)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "                train_losses_cifar, val_losses_cifar, val_accuracies_cifar = train_model(\n",
        "                    model=model_cifar,\n",
        "                    train_loader = train_dataloader,\n",
        "                    test_loader = val_dataloader,\n",
        "                    optimizer=optimizer_cifar,\n",
        "                    scheduler=scheduler_cifar,\n",
        "                    criterion=criterion,\n",
        "                    epochs=EPOCHS,\n",
        "                    hyperparameters=hyperparameters\n",
        "                )\n",
        "                if val_accuracies_cifar:\n",
        "                  max_accuracy = max(val_accuracies_cifar)\n",
        "                  best_epoch = val_accuracies_cifar.index(max_accuracy)\n",
        "                  best_train_loss_epoch = train_losses_cifar[best_epoch]\n",
        "                  best_val_loss_epoch = val_losses_cifar[best_epoch]\n",
        "\n",
        "                  print(f\"Configuration Results - LR={lr}, Momentum={momentum}, Weight Decay={weight_decay}, Batch Size={batch_size}\")\n",
        "                  print(f\"Final Train Loss: {train_losses_cifar[-1]:.4f}, Final Val Loss: {val_losses_cifar[-1]:.4f}, Max Val Accuracy: {max_accuracy:.4f}\")\n",
        "                  print(\"-\" * 50)\n",
        "                  if max_accuracy > best_accuracy:\n",
        "                      best_accuracy = max_accuracy\n",
        "                      best_hyperparameters = {\n",
        "                          'learning_rate': lr,\n",
        "                          'momentum': momentum,\n",
        "                          'weight_decay': weight_decay,\n",
        "                          'batch_size': batch_size\n",
        "                      }\n",
        "                      best_train_loss = best_train_loss_epoch\n",
        "                      best_val_loss = best_val_loss_epoch\n",
        "                else:\n",
        "                   print(f\"No validation accuracies for LR={lr}, Momentum={momentum}, Weight Decay={weight_decay}, Batch Size={batch_size}. Skipping...\")\n",
        "print(f\"Best Hyperparameters: {best_hyperparameters}, Best Validation Accuracy: {best_accuracy}\")\n",
        "\n",
        "BATCH_SIZE = best_hyperparameters['batch_size']\n",
        "LEARNING_RATE = best_hyperparameters['learning_rate']\n",
        "MOMENTUM = best_hyperparameters['momentum']\n",
        "WEIGHT_DECAY = best_hyperparameters['weight_decay']\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "test_loss, test_accuracy = evaluate_model(model_cifar, test_dataloader, criterion, torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "print(\"\\nFinal Model Evaluation on Test Set:\")\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(test_loss, label='CIFAR-100 Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_accuracy, label='CIFAR-100 Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCdLhW1Vv80"
      },
      "source": [
        "## FL Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bxpBSJyCoSPx"
      },
      "outputs": [],
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, epochs, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "      #print(f\"Client {self.client_id}, Epoch {epoch+1}/{epochs}\")\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KkSMDm61Acaf"
      },
      "outputs": [],
      "source": [
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "\n",
        "  def federated_averaging(self, epochs, batch_size, num_rounds, fraction_fit, skewness = None, hyperparameters = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "     # Carica il checkpoint se esiste\n",
        "    if skewness is  None:\n",
        "      start_epoch = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "    else:\n",
        "      start_epoch = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Skewed/\")\n",
        "\n",
        "    for round in range(start_epoch, num_rounds):\n",
        "\n",
        "\n",
        "      if skewness is not None:\n",
        "        probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False, p=probabilities)\n",
        "\n",
        "      else:\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False)\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, epochs, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model\n",
        "      loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True), nn.CrossEntropyLoss(), device)\n",
        "      self.round_losses.append(loss)\n",
        "      self.round_accuracies.append(accuracy)\n",
        "      print(f\"Round {round}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "      if skewness is  None:\n",
        "        save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\")\n",
        "      else:\n",
        "        save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Skewed/\")\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='CIFAR-100 Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='CIFAR-100 Test Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "nEh36xySq9Pr",
        "outputId": "4a353471-7ca5-4324-ac95-1601e34036e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-dcfae00a2cda>:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint found: Resume epoch 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-811f80c69ee0>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mserver_uniform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cifar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_EPOCHS}_C{C}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mserver_uniform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfederated_averaging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROUNDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-0e84cb7206c1>\u001b[0m in \u001b[0;36mfederated_averaging\u001b[0;34m(self, epochs, batch_size, num_rounds, fraction_fit, skewness, hyperparameters)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mclient_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_clients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mclient_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mnew_global_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobal_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-19f7a28ebe5b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, global_weights, epochs, batch_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "K = 100\n",
        "LOCAL_EPOCHS = 4 # J\n",
        "ROUNDS = 2000\n",
        "C = 0.1\n",
        "BATCH_SIZE = 16\n",
        "LR = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-3\n",
        "SKEWNESS=0.5\n",
        "\n",
        "optimizer_params = {\n",
        "    \"lr\": LR,\n",
        "    \"momentum\": MOMENTUM,\n",
        "    \"weight_decay\": WEIGHT_DECAY\n",
        "}\n",
        "\n",
        "model_cifar = LeNet5(100)\n",
        "\n",
        "train_dataset = CIFAR100Dataset(DIR_DATA, split='train', sharding='iid', K=K)\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "  client_data = Subset(train_dataset, train_dataset.data[train_dataset.data[\"client_id\"] == i].index)\n",
        "  clients.append(Client(model_cifar, i, client_data, optimizer_params))\n",
        "\n",
        "\n",
        "server_uniform = Server(model_cifar, clients, test_dataset)\n",
        "hyperparameters = f\"BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_EPOCHS}_C{C}\"\n",
        "server_uniform.federated_averaging(epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C,hyperparameters=hyperparameters)\n",
        "\n",
        "\n",
        "\n",
        "#server_skewed = Server(model_cifar, clients, test_dataset)\n",
        "#hyperparameters = f\"BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_EPOCHS}_C{C},SK{SKEWNESS}\"\n",
        "#server_skewed.federated_averaging(epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C, skewness=SKEWNESS, hyperparameters=hyperparameters)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}