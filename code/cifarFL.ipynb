{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK5E-uqb92ci"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LVs00uLvecJo"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import drive\n",
        "\n",
        "# Set the working directory\n",
        "DIR_DATA = '/content/'\n",
        "os.chdir(DIR_DATA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBqh7noxfUWv"
      },
      "source": [
        "# CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zbl-Cn0aw2CW"
      },
      "outputs": [],
      "source": [
        "class CIFAR100Dataset(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None, sharding=None, K=10, Nc=2):\n",
        "        \"\"\"\n",
        "        CIFAR-100 Dataset with IID and non-IID sharding.\n",
        "\n",
        "        Args:\n",
        "        - root (str): Directory to store the dataset.\n",
        "        - split (str): 'train' or 'test'.\n",
        "        - transform (callable): Transformations applied to the images.\n",
        "        - sharding (str): 'iid' or 'niid'.\n",
        "        - K (int): Number of clients for the sharding.\n",
        "        - Nc (int): Number of classes per client (used for non-iid sharding).\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.sharding = sharding\n",
        "        self.K = K\n",
        "        self.Nc = Nc\n",
        "\n",
        "        # Default transformations if none are provided\n",
        "        if self.transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # CIFAR-100 normalization\n",
        "        ])\n",
        "\n",
        "        dataset = datasets.CIFAR100(\n",
        "            root=self.root,\n",
        "            train=(self.split == 'train'),\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.data = pd.DataFrame({\n",
        "            \"image\": [dataset[i][0] for i in range(len(dataset))],\n",
        "            \"label\": [dataset[i][1] for i in range(len(dataset))]\n",
        "        })\n",
        "\n",
        "        if self.split == 'train' and self.sharding:\n",
        "            self.data = self._apply_sharding()\n",
        "\n",
        "    def _apply_sharding(self):\n",
        "        \"\"\"Apply IID or non-IID sharding to the training data.\"\"\"\n",
        "        if self.sharding == 'iid':\n",
        "            return self._iid_sharding()\n",
        "        elif self.sharding == 'niid':\n",
        "            return self._non_iid_sharding()\n",
        "        else:\n",
        "            raise ValueError(\"Sharding must be 'iid' or 'niid'.\")\n",
        "\n",
        "    def _iid_sharding(self):\n",
        "        \"\"\"Split data IID: uniformly distribute samples across K clients.\"\"\"\n",
        "        data_split = []\n",
        "        indices = self.data.index.tolist()\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        # Split indices equally among K clients\n",
        "        client_indices = [indices[i::self.K] for i in range(self.K)]\n",
        "\n",
        "        for client_id, idxs in enumerate(client_indices):\n",
        "            client_data = self.data.loc[idxs].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def _non_iid_sharding(self):\n",
        "        \"\"\"Split data non-IID: assign Nc classes per client.\"\"\"\n",
        "        data_split = []\n",
        "        unique_classes = self.data['label'].unique()\n",
        "        random.shuffle(unique_classes)\n",
        "\n",
        "        # Divide classes into groups of Nc\n",
        "        class_groups = [unique_classes[i:i + self.Nc] for i in range(0, len(unique_classes), self.Nc)]\n",
        "        class_groups = class_groups[:self.K]  # Limit to K clients\n",
        "\n",
        "        for client_id, class_group in enumerate(class_groups):\n",
        "            client_data = self.data[self.data['label'].isin(class_group)].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "\n",
        "            # Ensure approximately equal samples per client\n",
        "            client_data = client_data.sample(n=len(self.data) // self.K, replace=True, random_state=42)\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        image, label = row['image'], row['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet-5"
      ],
      "metadata": {
        "id": "_ptw5_Q9Yv4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=0)  # 3 input channels, 64 output channels\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=0)  # 64 input channels, 64 output channels\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(1600, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with activation and pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # Flatten the tensor\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers with activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "iekAEyoGYx0p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdRUiDbHz-ob"
      },
      "source": [
        "# Centralized training of CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RHgLtGILz8uS"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, test_losses, test_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1JKWoaq7ODR"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-3\n",
        "EPOCHS = 50\n",
        "\n",
        "train_dataset_big = CIFAR100Dataset(DIR_DATA, split='train')\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "\n",
        "indexes = range(0, len(train_dataset_big))\n",
        "splitting = train_test_split(indexes, train_size = 0.8, random_state = 42, stratify = train_dataset_big.data[\"label\"], shuffle = True)\n",
        "train_indexes = splitting[0]\n",
        "val_indexes = splitting[1]\n",
        "\n",
        "train_dataset = Subset(train_dataset_big, train_indexes)\n",
        "val_dataset = Subset(train_dataset_big, val_indexes)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "model_cifar = LeNet5(100)\n",
        "\n",
        "\n",
        "optimizer_cifar = optim.SGD(model_cifar.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler_cifar = CosineAnnealingLR(optimizer_cifar, T_max=200)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_losses_cifar, test_losses_cifar, test_accuracies_cifar = train_model(\n",
        "    model=model_cifar,\n",
        "    train_loader = train_dataloader,\n",
        "    test_loader = test_dataloader,\n",
        "    optimizer=optimizer_cifar,\n",
        "    scheduler=scheduler_cifar,\n",
        "    criterion=criterion,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(test_losses_cifar, label='CIFAR-100 Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(test_accuracies_cifar, label='CIFAR-100 Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCdLhW1Vv80"
      },
      "source": [
        "## FL Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bxpBSJyCoSPx"
      },
      "outputs": [],
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, epochs, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "      #print(f\"Client {self.client_id}, Epoch {epoch+1}/{epochs}\")\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KkSMDm61Acaf"
      },
      "outputs": [],
      "source": [
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "\n",
        "  def federated_averaging(self, epochs, batch_size, num_rounds, fraction_fit, skewness = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    for round in range(num_rounds):\n",
        "      print(f\"Round {round+1}/{num_rounds}\")\n",
        "\n",
        "      if skewness is not None:\n",
        "        probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False, p=probabilities)\n",
        "\n",
        "      else:\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False)\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, epochs, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model\n",
        "      loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True), nn.CrossEntropyLoss(), device)\n",
        "      self.round_losses.append(loss)\n",
        "      self.round_accuracies.append(accuracy)\n",
        "      print(f\"Round {round+1}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='CIFAR-100 Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='CIFAR-100 Test Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEh36xySq9Pr",
        "outputId": "9b1fce2e-ad8c-4460-9a2d-9846d8edd6ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Round 1/2000\n",
            "Round 1/2000 - Loss: 4.8450, Accuracy: 0.0895\n",
            "Round 2/2000\n",
            "Round 2/2000 - Loss: 5.4200, Accuracy: 0.0859\n",
            "Round 3/2000\n",
            "Round 3/2000 - Loss: 6.3286, Accuracy: 0.0931\n",
            "Round 4/2000\n",
            "Round 4/2000 - Loss: 5.7080, Accuracy: 0.0972\n",
            "Round 5/2000\n",
            "Round 5/2000 - Loss: 5.4106, Accuracy: 0.0940\n",
            "Round 6/2000\n",
            "Round 6/2000 - Loss: 5.8120, Accuracy: 0.1104\n",
            "Round 7/2000\n",
            "Round 7/2000 - Loss: 5.5504, Accuracy: 0.0905\n",
            "Round 8/2000\n",
            "Round 8/2000 - Loss: 5.5036, Accuracy: 0.1156\n",
            "Round 9/2000\n",
            "Round 9/2000 - Loss: 5.5604, Accuracy: 0.1085\n",
            "Round 10/2000\n",
            "Round 10/2000 - Loss: 5.3464, Accuracy: 0.1015\n",
            "Round 11/2000\n",
            "Round 11/2000 - Loss: 5.8827, Accuracy: 0.1100\n",
            "Round 12/2000\n",
            "Round 12/2000 - Loss: 5.6045, Accuracy: 0.1142\n",
            "Round 13/2000\n",
            "Round 13/2000 - Loss: 5.5105, Accuracy: 0.1150\n",
            "Round 14/2000\n",
            "Round 14/2000 - Loss: 5.4570, Accuracy: 0.1123\n",
            "Round 15/2000\n",
            "Round 15/2000 - Loss: 5.4409, Accuracy: 0.1239\n",
            "Round 16/2000\n",
            "Round 16/2000 - Loss: 5.5320, Accuracy: 0.1212\n",
            "Round 17/2000\n",
            "Round 17/2000 - Loss: 5.5037, Accuracy: 0.1166\n",
            "Round 18/2000\n",
            "Round 18/2000 - Loss: 5.4499, Accuracy: 0.1223\n",
            "Round 19/2000\n",
            "Round 19/2000 - Loss: 5.4983, Accuracy: 0.1241\n",
            "Round 20/2000\n",
            "Round 20/2000 - Loss: 5.3345, Accuracy: 0.1267\n",
            "Round 21/2000\n",
            "Round 21/2000 - Loss: 5.3794, Accuracy: 0.1219\n",
            "Round 22/2000\n",
            "Round 22/2000 - Loss: 5.2666, Accuracy: 0.1285\n",
            "Round 23/2000\n",
            "Round 23/2000 - Loss: 5.3630, Accuracy: 0.1313\n",
            "Round 24/2000\n",
            "Round 24/2000 - Loss: 5.2880, Accuracy: 0.1386\n",
            "Round 25/2000\n",
            "Round 25/2000 - Loss: 5.2825, Accuracy: 0.1364\n",
            "Round 26/2000\n",
            "Round 26/2000 - Loss: 5.2708, Accuracy: 0.1536\n",
            "Round 27/2000\n",
            "Round 27/2000 - Loss: 5.2598, Accuracy: 0.1342\n",
            "Round 28/2000\n",
            "Round 28/2000 - Loss: 5.0749, Accuracy: 0.1326\n",
            "Round 29/2000\n",
            "Round 29/2000 - Loss: 5.0822, Accuracy: 0.1482\n",
            "Round 30/2000\n",
            "Round 30/2000 - Loss: 5.2499, Accuracy: 0.1450\n",
            "Round 31/2000\n",
            "Round 31/2000 - Loss: 5.0616, Accuracy: 0.1424\n",
            "Round 32/2000\n",
            "Round 32/2000 - Loss: 5.0491, Accuracy: 0.1557\n",
            "Round 33/2000\n",
            "Round 33/2000 - Loss: 5.0898, Accuracy: 0.1603\n",
            "Round 34/2000\n",
            "Round 34/2000 - Loss: 5.1576, Accuracy: 0.1539\n",
            "Round 35/2000\n",
            "Round 35/2000 - Loss: 4.9826, Accuracy: 0.1686\n",
            "Round 36/2000\n",
            "Round 36/2000 - Loss: 5.2015, Accuracy: 0.1461\n",
            "Round 37/2000\n",
            "Round 37/2000 - Loss: 4.9002, Accuracy: 0.1519\n",
            "Round 38/2000\n",
            "Round 38/2000 - Loss: 4.8763, Accuracy: 0.1651\n",
            "Round 39/2000\n",
            "Round 39/2000 - Loss: 5.1026, Accuracy: 0.1483\n",
            "Round 40/2000\n",
            "Round 40/2000 - Loss: 5.1848, Accuracy: 0.1649\n",
            "Round 41/2000\n",
            "Round 41/2000 - Loss: 4.9595, Accuracy: 0.1461\n",
            "Round 42/2000\n",
            "Round 42/2000 - Loss: 5.0826, Accuracy: 0.1423\n",
            "Round 43/2000\n"
          ]
        }
      ],
      "source": [
        "K = 100\n",
        "LOCAL_EPOCHS = 4 # J\n",
        "ROUNDS = 2000\n",
        "C = 0.1\n",
        "BATCH_SIZE = 16\n",
        "LR = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-3\n",
        "\n",
        "optimizer_params = {\n",
        "    \"lr\": LR,\n",
        "    \"momentum\": MOMENTUM,\n",
        "    \"weight_decay\": WEIGHT_DECAY\n",
        "}\n",
        "\n",
        "model_cifar = LeNet5(100)\n",
        "\n",
        "train_dataset = CIFAR100Dataset(DIR_DATA, split='train', sharding='iid', K=K)\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "  client_data = Subset(train_dataset, train_dataset.data[train_dataset.data[\"client_id\"] == i].index)\n",
        "  clients.append(Client(model_cifar, i, client_data, optimizer_params))\n",
        "\n",
        "server_uniform = Server(model_cifar, clients, test_dataset)\n",
        "\n",
        "server_uniform.federated_averaging(epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C)\n",
        "\n",
        "#server_skewed = Server(model_cifar, clients, test_dataset)\n",
        "\n",
        "#server_skewed.federated_averaging(epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C, skewness=0.5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}