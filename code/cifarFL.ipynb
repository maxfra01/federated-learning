{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK5E-uqb92ci"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "LVs00uLvecJo"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Third-party library imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvuWze6kjxhV"
      },
      "source": [
        "## Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UihnNmjPjxhV",
        "outputId": "20a6297e-33d6-4390-eca3-4cb2943afa43"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "DIR_DATA = \"../data\"\n",
        "CHECKPOINT_DIR = '../checkpoints'\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, hyperparameters, subfolder=\"\", data_to_save=None):\n",
        "    \"\"\"Salva il checkpoint del modello e rimuove quello precedente.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "    # File corrente e precedente\n",
        "    filename = f\"model_epoch_{epoch}_params_{hyperparameters}.pth\"\n",
        "    filepath = os.path.join(subfolder_path, filename)\n",
        "    filename_json = f\"model_epoch_{epoch}_params_{hyperparameters}.json\"\n",
        "    filepath_json = os.path.join(subfolder_path, filename_json)\n",
        "\n",
        "\n",
        "    previous_filename = f\"model_epoch_{epoch -1}_params_{hyperparameters}.pth\"\n",
        "    previous_filepath = os.path.join(subfolder_path, previous_filename)\n",
        "    previous_filename_json = f\"model_epoch_{epoch -1}_params_{hyperparameters}.json\"\n",
        "    previous_filepath_json = os.path.join(subfolder_path, previous_filename_json)\n",
        "\n",
        "    # Rimuove il checkpoint precedente\n",
        "    if epoch > 1 and os.path.exists(previous_filepath) and os.path.exists(previous_filepath_json):\n",
        "        os.remove(previous_filepath)\n",
        "        os.remove(previous_filepath_json)\n",
        "\n",
        "    # Salva il nuovo checkpoint\n",
        "    if optimizer is not None:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),  # Salvataggio dello stato dell'ottimizzatore\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    else:\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, filepath)\n",
        "    print(f\"Checkpoint salvato: {filepath}\")\n",
        "\n",
        "    with open(filepath_json, 'w') as json_file:\n",
        "      json.dump(data_to_save, json_file, indent=4)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, hyperparameters, subfolder=\"\"):\n",
        "    \"\"\"Carica l'ultimo checkpoint disponibile basato sugli iperparametri.\"\"\"\n",
        "    subfolder_path = os.path.join(CHECKPOINT_DIR, subfolder)\n",
        "    if not os.path.exists(subfolder_path):\n",
        "        print(\"No checkpoint found, Starting now...\")\n",
        "        return 1, None  # Le epoche iniziano da 1\n",
        "\n",
        "    # Cerca i file con gli iperparametri specificati\n",
        "    files = [f for f in os.listdir(subfolder_path) if f\"params_{hyperparameters}\" in f and f.endswith('.pth')]\n",
        "    if files:\n",
        "        # Trova il file con l'epoca più alta\n",
        "        latest_file = max(files, key=lambda x: int(x.split('_')[2]))\n",
        "        filepath = os.path.join(subfolder_path, latest_file)\n",
        "        checkpoint = torch.load(filepath)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Trova e carica il file JSON associato\n",
        "        json_filename = latest_file.replace('.pth', '.json')\n",
        "        json_filepath = os.path.join(subfolder_path, json_filename)\n",
        "        json_data = None\n",
        "        if os.path.exists(json_filepath):\n",
        "            with open(json_filepath, 'r') as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "            print(f\"JSON data loaded: {json_filepath}\")\n",
        "        else:\n",
        "            print(f\"No JSON file found for: {latest_file}\")\n",
        "\n",
        "        print(f\"Checkpoint found: Resume epoch {checkpoint['epoch'] + 1}\")\n",
        "        return checkpoint['epoch'] + 1, json_data\n",
        "\n",
        "    print(\"No checkpoint found, Starting now...\")\n",
        "    return 1, None  # Le epoche iniziano da 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBqh7noxfUWv"
      },
      "source": [
        "# CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Zbl-Cn0aw2CW"
      },
      "outputs": [],
      "source": [
        "class CIFAR100Dataset(Dataset):\n",
        "    def __init__(self, root, split='train', transform=None, sharding=None, K=10, Nc=2):\n",
        "        \"\"\"\n",
        "        CIFAR-100 Dataset with IID and non-IID sharding.\n",
        "\n",
        "        Args:\n",
        "        - root (str): Directory to store the dataset.\n",
        "        - split (str): 'train' or 'test'.\n",
        "        - transform (callable): Transformations applied to the images.\n",
        "        - sharding (str): 'iid' or 'niid'.\n",
        "        - K (int): Number of clients for the sharding.\n",
        "        - Nc (int): Number of classes per client (used for non-iid sharding).\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.sharding = sharding\n",
        "        self.K = K\n",
        "        self.Nc = Nc\n",
        "\n",
        "        # Default transformations if none are provided\n",
        "        if self.transform is None:\n",
        "            if self.split == 'train':\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.RandomResizedCrop(24, scale=(0.8, 1.0)),\n",
        "                    transforms.RandomHorizontalFlip(),  # Flip orizzontale casuale\n",
        "                    transforms.RandomRotation(10),\n",
        "                    transforms.ToTensor(),  # Converte l'immagine in un tensore PyTorch\n",
        "                    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # Normalizzazione\n",
        "                ])\n",
        "            else:\n",
        "                self.transform = transforms.Compose([\n",
        "                    transforms.CenterCrop(24),\n",
        "                    transforms.ToTensor(),  # Converte in tensore PyTorch\n",
        "                    transforms.Normalize(mean=[0.5071, 0.4865, 0.4409], std=[0.2673, 0.2564, 0.2762]),  # Normalizzazione\n",
        "                ])\n",
        "\n",
        "        dataset = datasets.CIFAR100(\n",
        "            root=self.root,\n",
        "            train=(self.split == 'train'),\n",
        "            download=True\n",
        "        )\n",
        "\n",
        "        self.data = pd.DataFrame({\n",
        "            \"image\": [dataset[i][0] for i in range(len(dataset))],\n",
        "            \"label\": [dataset[i][1] for i in range(len(dataset))]\n",
        "        })\n",
        "\n",
        "        if self.split == 'train' and self.sharding:\n",
        "            self.data = self._apply_sharding()\n",
        "\n",
        "    def _apply_sharding(self):\n",
        "        \"\"\"Apply IID or non-IID sharding to the training data.\"\"\"\n",
        "        if self.sharding == 'iid':\n",
        "            return self._iid_sharding()\n",
        "        elif self.sharding == 'niid':\n",
        "            return self._non_iid_sharding()\n",
        "        else:\n",
        "            raise ValueError(\"Sharding must be 'iid' or 'niid'.\")\n",
        "\n",
        "    def _iid_sharding(self):\n",
        "        \"\"\"Split data IID: uniformly distribute samples across K clients.\"\"\"\n",
        "        data_split = []\n",
        "        indices = self.data.index.tolist()\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        # Split indices equally among K clients\n",
        "        client_indices = [indices[i::self.K] for i in range(self.K)]\n",
        "\n",
        "        for client_id, idxs in enumerate(client_indices):\n",
        "            client_data = self.data.loc[idxs].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def _non_iid_sharding(self):\n",
        "        \"\"\"Split data non-IID: assign Nc classes per client.\"\"\"\n",
        "        data_split = []\n",
        "        unique_classes = self.data['label'].unique()\n",
        "        random.shuffle(unique_classes)\n",
        "\n",
        "        # Divide classes into groups of Nc\n",
        "        class_groups = [unique_classes[i:i + self.Nc] for i in range(0, len(unique_classes), self.Nc)]\n",
        "        class_groups = class_groups[:self.K]  # Limit to K clients\n",
        "\n",
        "        for client_id, class_group in enumerate(class_groups):\n",
        "            client_data = self.data[self.data['label'].isin(class_group)].copy()\n",
        "            client_data['client_id'] = client_id\n",
        "\n",
        "            # Ensure approximately equal samples per client\n",
        "            client_data = client_data.sample(n=len(self.data) // self.K, replace=True, random_state=42)\n",
        "            data_split.append(client_data)\n",
        "\n",
        "        return pd.concat(data_split, ignore_index=True)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        image, label = row['image'], row['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ptw5_Q9Yv4u"
      },
      "source": [
        "## LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "iekAEyoGYx0p"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self,num_classes=100):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 64, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64 * 3 * 3, 384),  # Updated to be consistent with data augmentation\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(384, 192),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(192, num_classes)  # 100 classes for CIFAR-100\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output of the conv layers\n",
        "        x = self.fc_layer(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdRUiDbHz-ob"
      },
      "source": [
        "# Centralized training of CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "RHgLtGILz8uS"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses, test_losses, test_accuracies = [], [], []\n",
        "\n",
        "    # Carica checkpoint se esiste\n",
        "    start_epoch, json_data = load_checkpoint(model, optimizer, hyperparameters, \"Centralized/\")\n",
        "    if json_data is not None:\n",
        "        test_losses = json_data.get('test_losses', [])\n",
        "        test_accuracies = json_data.get('test_accuracies', [])\n",
        "        train_losses = json_data.get('train_losses', [])\n",
        "\n",
        "    if start_epoch >= epochs:\n",
        "        print(f\"Checkpoint trovato, configurazione già completata. Valutazione solo sul test set.\")\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        return train_losses, test_losses, test_accuracies\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Salva checkpoint\n",
        "        save_checkpoint(model, optimizer, epoch, hyperparameters, \"Centralized/\")\n",
        "\n",
        "        # Valutazione sul validation set\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs}, Train Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, test_losses, test_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return total_loss / len(test_loader), correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1JKWoaq7ODR",
        "outputId": "787fc84c-74b7-457f-ba92-e14bfba5642c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Testing configuration: LR=0.001, Momentum=0.9, Weight Decay=0.001, Batch Size=64\n",
            "No checkpoint found, Starting now...\n",
            "Checkpoint salvato: ../checkpoints\\Centralized/model_epoch_1_params_BS64_LR0.001_WD0.001_M0.9.pth\n",
            "Epoch 1/50, Train Loss: 3589.9044, Test Loss: 4.5747, Test Accuracy: 0.0324\n",
            "Checkpoint salvato: ../checkpoints\\Centralized/model_epoch_2_params_BS64_LR0.001_WD0.001_M0.9.pth\n",
            "Epoch 2/50, Train Loss: 3446.5025, Test Loss: 4.2285, Test Accuracy: 0.0516\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[118], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m scheduler_cifar \u001b[38;5;241m=\u001b[39m CosineAnnealingLR(optimizer_cifar, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     28\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 30\u001b[0m train_losses_cifar, test_losses_cifar, test_accuracies_cifar \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_cifar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_cifar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_cifar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_accuracies_cifar:\n\u001b[0;32m     42\u001b[0m     max_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(test_accuracies_cifar)\n",
            "Cell \u001b[1;32mIn[117], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs, hyperparameters)\u001b[0m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     23\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[115], line 102\u001b[0m, in \u001b[0;36mCIFAR100Dataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     99\u001b[0m image, label \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m--> 102\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:922\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    920\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    921\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCHS = 50\n",
        "LEARNING_RATES = [0.001, 0.01, 0.1]\n",
        "MOMENTUMS = [0.9]\n",
        "WEIGHT_DECAYS = [1e-3, 1e-4, 1e-5]\n",
        "BATCH_SIZES = [64]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n",
        "best_train_loss = float('inf')\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "train_dataset = CIFAR100Dataset(DIR_DATA, split='train')\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "\n",
        "for lr in LEARNING_RATES:\n",
        "    for momentum in MOMENTUMS:\n",
        "        for weight_decay in WEIGHT_DECAYS:\n",
        "            for batch_size in BATCH_SIZES:\n",
        "                hyperparameters = f\"BS{batch_size}_LR{lr}_WD{weight_decay}_M{momentum}\"\n",
        "                print(f\"Testing configuration: LR={lr}, Momentum={momentum}, Weight Decay={weight_decay}, Batch Size={batch_size}\")\n",
        "\n",
        "                train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True,pin_memory=True)\n",
        "                test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0,pin_memory=True)\n",
        "\n",
        "                model_cifar = LeNet5(100)\n",
        "                optimizer_cifar = optim.SGD(model_cifar.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "                scheduler_cifar = CosineAnnealingLR(optimizer_cifar, T_max=200)\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "                train_losses_cifar, test_losses_cifar, test_accuracies_cifar = train_model(\n",
        "                    model=model_cifar,\n",
        "                    train_loader=train_dataloader,\n",
        "                    test_loader=test_dataloader,\n",
        "                    optimizer=optimizer_cifar,\n",
        "                    scheduler=scheduler_cifar,\n",
        "                    criterion=criterion,\n",
        "                    epochs=EPOCHS,\n",
        "                    hyperparameters=hyperparameters\n",
        "                )\n",
        "\n",
        "                if test_accuracies_cifar:\n",
        "                    max_accuracy = max(test_accuracies_cifar)\n",
        "                    best_epoch = test_accuracies_cifar.index(max_accuracy)\n",
        "                    best_train_loss_epoch = train_losses_cifar[best_epoch]\n",
        "                    best_test_loss_epoch = test_losses_cifar[best_epoch]\n",
        "\n",
        "                    print(f\"Configuration Results - LR={lr}, Momentum={momentum}, Weight Decay={weight_decay}, Batch Size={batch_size}\")\n",
        "                    print(f\"Train Loss at Max Accuracy: {best_train_loss_epoch:.4f}, Test Loss at Max Accuracy: {best_test_loss_epoch:.4f}, Max Test Accuracy: {max_accuracy:.4f}\")\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                    if max_accuracy > best_accuracy:\n",
        "                        best_accuracy = max_accuracy\n",
        "                        best_hyperparameters = {\n",
        "                            'learning_rate': lr,\n",
        "                            'momentum': momentum,\n",
        "                            'weight_decay': weight_decay,\n",
        "                            'batch_size': batch_size\n",
        "                        }\n",
        "                        best_train_loss = best_train_loss_epoch\n",
        "                        best_test_loss = best_test_loss_epoch\n",
        "                else:\n",
        "                    print(f\"No test accuracies for LR={lr}, Momentum={momentum}, Weight Decay={weight_decay}, Batch Size={batch_size}. Skipping...\")\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_hyperparameters}, Best Test Accuracy: {best_accuracy}\")\n",
        "\n",
        "# Plotting and saving results for the best configuration\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, EPOCHS + 1), test_losses_cifar, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Test Loss per Epoch')\n",
        "plt.legend()\n",
        "plt.savefig('best_test_loss.png')\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, EPOCHS + 1), test_accuracies_cifar, label='Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy per Epoch')\n",
        "plt.legend()\n",
        "plt.savefig('best_test_accuracy.png')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgqkHxRbdJK-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oCdLhW1Vv80"
      },
      "source": [
        "## FL Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "bxpBSJyCoSPx"
      },
      "outputs": [],
      "source": [
        "def generate_skewed_probabilities(num_clients, gamma):\n",
        "    \"\"\"It generates skewed probabilities for clients using a Dirichlet distribution.\"\"\"\n",
        "    probabilities = np.random.dirichlet([gamma] * num_clients)\n",
        "    return probabilities\n",
        "\n",
        "def plot_selected_clients_distribution(selected_clients_per_round, num_clients, hyperparameters):\n",
        "    \"\"\"Plotta la distribuzione dei client selezionati alla fine del processo.\"\"\"\n",
        "    counts = np.zeros(num_clients)\n",
        "\n",
        "    # Conta quante volte ogni client è stato selezionato in tutti i round\n",
        "    for selected_clients in selected_clients_per_round:\n",
        "        for client in selected_clients:\n",
        "            counts[client] += 1\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(num_clients), counts, color='skyblue', edgecolor='black')\n",
        "    plt.title(\"Distribuzione dei Client Selezionati Durante il Federated Averaging\")\n",
        "    plt.xlabel(\"Client ID\")\n",
        "    plt.ylabel(\"Frequenza di Selezione\")\n",
        "    plt.grid(axis='y')\n",
        "    plt.savefig(f\"CIFAR100_Client_distribution_{hyperparameters}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class Client:\n",
        "\n",
        "  def __init__(self, model, client_id, data, optimizer_params):\n",
        "    self.client_id = client_id\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.optimizer_params = optimizer_params\n",
        "\n",
        "  def train(self, global_weights, epochs, batch_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "    self.model.load_state_dict(global_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(\n",
        "        self.model.parameters(),\n",
        "        lr=self.optimizer_params['lr'],\n",
        "        momentum=self.optimizer_params['momentum'],\n",
        "        weight_decay=self.optimizer_params['weight_decay']\n",
        "        )\n",
        "    trainloader = DataLoader(self.data, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
        "    for epoch in range(epochs):\n",
        "      #print(f\"Client {self.client_id}, Epoch {epoch+1}/{epochs}\")\n",
        "      for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return self.model.state_dict()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "KkSMDm61Acaf"
      },
      "outputs": [],
      "source": [
        "class Server:\n",
        "\n",
        "  def __init__(self, model, clients, test_data):\n",
        "    self.model = model\n",
        "    self.clients = clients\n",
        "    self.test_data = test_data\n",
        "    self.round_losses = []\n",
        "    self.round_accuracies = []\n",
        "    self.selected_clients_per_round = [] #clint selezionati per skewness\n",
        "\n",
        "  def federated_averaging(self, epochs, batch_size, num_rounds, fraction_fit, skewness = None, hyperparameters = None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model.to(device)\n",
        "     # Carica il checkpoint se esiste\n",
        "    data_to_load = None\n",
        "    if skewness is  None:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Uniform/\")\n",
        "    else:\n",
        "      start_epoch, data_to_load = load_checkpoint(self.model,optimizer=None,hyperparameters=hyperparameters, subfolder=\"Federated_Skewed/\")\n",
        "\n",
        "    if data_to_load is not None:\n",
        "      self.round_losses = data_to_load['round_losses']\n",
        "      self.round_accuracies = data_to_load['round_accuracies']\n",
        "      self.selected_clients_per_round = data_to_load['selected_clients_per_round']\n",
        "\n",
        "\n",
        "    for round in range(start_epoch, num_rounds+1):\n",
        "\n",
        "      if skewness is not None:\n",
        "        probabilities = generate_skewed_probabilities(len(self.clients), skewness)\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False, p=probabilities)\n",
        "\n",
        "      else:\n",
        "        selected_clients = np.random.choice(self.clients, size=max(1, int(fraction_fit*len(self.clients))), replace=False)\n",
        "\n",
        "      self.selected_clients_per_round.append([client.client_id for client in selected_clients])\n",
        "\n",
        "\n",
        "      global_weights = self.model.state_dict()\n",
        "\n",
        "      # Simulating parallel clients training\n",
        "      client_weights = {}\n",
        "      for client in selected_clients:\n",
        "        client_weights[client.client_id] = client.train(global_weights, epochs, batch_size)\n",
        "\n",
        "      new_global_weights = {key: torch.zeros_like(value).type(torch.float32) for key, value in global_weights.items()}\n",
        "\n",
        "      total_data_size = sum([len(client.data) for client in selected_clients])\n",
        "      for client in selected_clients:\n",
        "        scaling_factor = len(client.data) / total_data_size\n",
        "        for key in new_global_weights.keys():\n",
        "          new_global_weights[key] += scaling_factor * client_weights[client.client_id][key]\n",
        "\n",
        "      # Update global model weights\n",
        "      self.model.load_state_dict(new_global_weights)\n",
        "\n",
        "      # Evaluate global model\n",
        "      loss, accuracy = evaluate_model(self.model, DataLoader(self.test_data, batch_size=batch_size, shuffle=True,pin_memory=True), nn.CrossEntropyLoss(), device)\n",
        "      self.round_losses.append(loss)\n",
        "      self.round_accuracies.append(accuracy)\n",
        "      print(f\"Round {round}/{num_rounds} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "      data_to_save = {\n",
        "        'round_losses': self.round_losses,\n",
        "        'round_accuracies': self.round_accuracies,\n",
        "        'selected_clients_per_round': [[client for client in round_clients] for round_clients in self.selected_clients_per_round]  # Serializziamo solo i client_id\n",
        "    }\n",
        "\n",
        "      if skewness is  None:\n",
        "        save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Uniform/\", data_to_save)\n",
        "      else:\n",
        "        save_checkpoint(self.model, None, round , hyperparameters, \"Federated_Skewed/\", data_to_save)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(self.round_losses, label='CIFAR-100 Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.round_accuracies, label='CIFAR-100 Test Accuracy')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    if skewness is  None:\n",
        "      plt.savefig(f\"CIFAR100_fedavg_uniform{hyperparameters}.jpg\")\n",
        "    else:\n",
        "      plt.savefig(f\"CIFAR100_fedavg_skew{hyperparameters}.jpg\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    plot_selected_clients_distribution(self.selected_clients_per_round, len(self.clients), hyperparameters)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nEh36xySq9Pr",
        "outputId": "d4d2d36a-92d4-430b-de85-5edb1593415f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_25084\\2921074611.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSON data loaded: ../checkpoints\\Federated_Skewed/model_epoch_850_params_BS64_LR0.001_M0.9_WD0.001_J4_C0.1_SK0.01.json\n",
            "Checkpoint found: Resume epoch 851\n"
          ]
        }
      ],
      "source": [
        "K = 100 #fix\n",
        "LOCAL_EPOCHS = 4 # J\n",
        "ROUNDS = 2000\n",
        "C = 0.1 #fix\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-3\n",
        "SKEWNESS=0.01\n",
        "\n",
        "optimizer_params = {\n",
        "    \"lr\": LR,\n",
        "    \"momentum\": MOMENTUM,\n",
        "    \"weight_decay\": WEIGHT_DECAY\n",
        "}\n",
        "\n",
        "model_cifar = LeNet5(100)\n",
        "\n",
        "train_dataset = CIFAR100Dataset(DIR_DATA, split='train', sharding='iid', K=K)\n",
        "test_dataset = CIFAR100Dataset(DIR_DATA, split='test')\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "  client_data = Subset(train_dataset, train_dataset.data[train_dataset.data[\"client_id\"] == i].index)\n",
        "  clients.append(Client(model_cifar, i, client_data, optimizer_params))\n",
        "\n",
        "\n",
        "#server_uniform = Server(model_cifar, clients, test_dataset)\n",
        "#hyperparameters = f\"BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_EPOCHS}_C{C}\"\n",
        "#server_uniform.federated_averaging(epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C,hyperparameters=hyperparameters)\n",
        "\n",
        "\n",
        "\n",
        "server_skewed = Server(model_cifar, clients, test_dataset)\n",
        "hyperparameters = f\"BS{BATCH_SIZE}_LR{LR}_M{MOMENTUM}_WD{WEIGHT_DECAY}_J{LOCAL_EPOCHS}_C{C}_SK{SKEWNESS}\"\n",
        "server_skewed.federated_averaging(epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, num_rounds=ROUNDS, fraction_fit=C, skewness=SKEWNESS, hyperparameters=hyperparameters)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
